{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caf4ff42"
   },
   "source": [
    "# Reorganized Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ae86981"
   },
   "source": [
    "## 1. Load the required data and models from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1587271b"
   },
   "source": [
    "**Reasoning**:\n",
    "Load the necessary data (tokenized texts and dates), the trained LDA model, and the corpus from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "executionInfo": {
     "elapsed": 37369,
     "status": "ok",
     "timestamp": 1756640813194,
     "user": {
      "displayName": "Rafsun Sheikh",
      "userId": "00661906799562681997"
     },
     "user_tz": -600
    },
    "id": "jT6TX_-XeY7X",
    "outputId": "35b3299f-9f4e-45c1-c085-fc657341701f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.1\n",
      "    Uninstalling scipy-1.16.1:\n",
      "      Successfully uninstalled scipy-1.16.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "44026d08139842c5a4603476306ba2d0",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37982,
     "status": "ok",
     "timestamp": 1756640859305,
     "user": {
      "displayName": "Rafsun Sheikh",
      "userId": "00661906799562681997"
     },
     "user_tz": -600
    },
    "id": "Ihyg1Y0QeXzd",
    "outputId": "9ff8bebe-8390-42f3-cf93-8b1cc714f787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "89fed5f2",
    "outputId": "ea3bb0ac-59bb-40b5-d8e3-9fcef3359ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model loaded successfully from /content/drive/MyDrive/Colab Notebooks/alex/models/lda_30_topics/lda_model_30_topics.gensim\n",
      "Corpus loaded successfully from /content/drive/MyDrive/Colab Notebooks/alex/models/dictionary_and_corpus/corpus.mm\n",
      "Excel file loaded successfully from /content/drive/MyDrive/Colab Notebooks/alex/tokenized_newspaper_data.xlsx\n",
      "Extracted 150219 documents for texts variable.\n",
      "\n",
      "'df_dominant_topic' DataFrame created.\n",
      "Year column added to df_dominant_topic from 'date' column.\n",
      "\n",
      "Proceeding with filtering by year.\n",
      "Unique years found: [2002. 2003. 2004. 2006. 2007. 2009. 2010. 2011. 2012. 2013. 2016. 2017.\n",
      " 2018. 2019. 2021. 2023. 2024. 2025. 2005. 2008. 2014. 2015. 2020. 2022.]\n",
      "Filtered dataframe for year 2002.0 with 9158 documents.\n",
      "Filtered dataframe for year 2003.0 with 10338 documents.\n",
      "Filtered dataframe for year 2004.0 with 9014 documents.\n",
      "Filtered dataframe for year 2006.0 with 8764 documents.\n",
      "Filtered dataframe for year 2007.0 with 7396 documents.\n",
      "Filtered dataframe for year 2009.0 with 6762 documents.\n",
      "Filtered dataframe for year 2010.0 with 6744 documents.\n",
      "Filtered dataframe for year 2011.0 with 6111 documents.\n",
      "Filtered dataframe for year 2012.0 with 6461 documents.\n",
      "Filtered dataframe for year 2013.0 with 5534 documents.\n",
      "Filtered dataframe for year 2016.0 with 5377 documents.\n",
      "Filtered dataframe for year 2017.0 with 4680 documents.\n",
      "Filtered dataframe for year 2018.0 with 4366 documents.\n",
      "Filtered dataframe for year 2019.0 with 5304 documents.\n",
      "Filtered dataframe for year 2021.0 with 4952 documents.\n",
      "Filtered dataframe for year 2023.0 with 4550 documents.\n",
      "Filtered dataframe for year 2024.0 with 5617 documents.\n",
      "Filtered dataframe for year 2025.0 with 3714 documents.\n",
      "Filtered dataframe for year 2005.0 with 8541 documents.\n",
      "Filtered dataframe for year 2008.0 with 7059 documents.\n",
      "Filtered dataframe for year 2014.0 with 5372 documents.\n",
      "Filtered dataframe for year 2015.0 with 5368 documents.\n",
      "Filtered dataframe for year 2020.0 with 4348 documents.\n",
      "Filtered dataframe for year 2022.0 with 3881 documents.\n",
      "\n",
      "Displaying head of dataframe for year 2002.0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    print(\\\"Skipping processing texts as yearly_dataframes are not available\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Document_No\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 46,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          46,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dominant_Topic\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 16,\n        \"max\": 22,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          16,\n          22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_Perc_Contrib\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"0.2309\",\n        \"max\": \"0.5945\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0.2309\",\n          \"0.5945\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Keywords\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"u, world, olympic, 2023, championship, united, golf, woman, first, olympics\",\n          \"said, like, would, one, get, going, time, year, think, know\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"['see', 'sidebar', '02c', 'ruston', 'la', 'dining', 'room', 'louisiana', 'cajun', 'cafe', 'crowded', 'leon', 'barmore', 'decides', 'conduct', 'predinner', 'tour', 'hometown', 'car', '15minute', 'trip', 'cover', 'universe', '53', '57', 'year', 'six', 'mile', 'road', 'farm', 'grew', 'preferring', 'shoot', 'basket', 'imitate', 'bob', 'cousy', 'rather', 'hunt', 'squirrel', 'pick', 'pea', 'ruston', 'high', 'made', 'allstate', 'basketball', 'played', 'two', 'statetitle', 'team', 'coached', 'became', 'basketball', 'star', 'across', 'street', 'louisiana', 'tech', 'earned', 'two', 'degree', 'met', 'wife', '36', 'year', 'speech', 'class', 'sent', 'daughter', 'college', 'also', 'barmore', 'coached', 'woman', 'basketball', 'team', 'national', 'prominence', '21', 'ncaa', 'tournament', 'lady', 'techsters', 'play', 'friday', 'calsanta', 'barbara', 'story', 'boy', 'made', 'good', 'locally', 'also', 'story', 'college', 'sport', 'anomaly', 'louisiana', 'tech', 'power', 'prencaa', 'day', 'woman', 'basketball', 'remains', 'elite', 'program', 'despite', 'balance', 'power', 'shifting', 'large', 'state', 'school', 'big', 'budget', 'tech', 'old', 'dominion', 'school', 'win', 'association', 'intercollegiate', 'athletics', 'woman', 'ncaa', 'title', 'isolated', 'underfunded', 'without', 'powerconference', 'affiliation', 'competing', 'new', 'power', 'connecticut', 'iowa', 'state', 'oklahoma', 'tech', 'recruiting', 'disadvantage', 'also', '3500', 'average', 'attendance', 'fan', 'loyal', 'buried', 'tech', 'souvenir', 'coffin', 'location', 'close', 'talent', 'base', 'however', 'biggest', 'reason', 'staying', 'power', 'barmore', 'powerful', 'connection', 'school', 'town', 'fierce', 'defense', 'program', 'tradition', 'louisiana', 'tech', 'say', 'family', 'take', 'louisiana', 'tech', 'man', 'also', 'take', 'man', 'louisiana', 'tech', 'turned', 'attractive', 'job', 'college', 'pro', 'stay', 'home', '20year', 'mark', '576', '86', '870', 'winning', 'percentage', 'game', 'best', 'two', 'year', 'ago', 'retired', '17', 'day', 'school', 'could', 'find', 'suitable', 'replacement', 'asked', 'return', 'could', 'say', 'anything', 'player', 'senior', 'guard', 'brooke', 'lassiter', 'say', 'forward', 'ayana', 'walker', 'highly', 'recruited', 'high', 'school', 'chose', 'tech', 'north', 'carolina', 'closer', 'home', 'houston', 'lassiter', 'grew', 'tech', 'fan', 'two', 'hour', 'away', 'portland', 'ark', 'committed', 'junior', 'year', 'family', 'come', 'game', 'chance', 'final', 'four', 'every', 'year', 'lassiter', 'say', 'combination', 'beat', 'program', 'introduced', 'tradition', 'want', 'know', 'want', 'desire', 'keep', 'going', 'keeping', 'going', 'gotten', 'harder', 'phenomenal', 'leon', 'able', 'program', 'say', 'van', 'chancellor', 'coach', 'wnba', 'champion', 'houston', 'comet', 'truly', 'amazed', 'able', 'maintain', 'say', '2000', 'olympic', 'coach', 'nell', 'fortner', 'former', 'tech', 'assistant', 'coach', 'wnba', 'indiana', 'fever', 'everything', 'program', 'one', 'best', 'teacher', 'game', 'adapts', 'talent', 'priority', 'straight', 'family', 'man', 'religious', 'man', 'love', 'player', 'thing', 'get', 'whack', 'producing', 'result', 'barmore', 'pass', 'credit', 'around', 'program', 'started', 'president', 'jay', 'taylor', 'coach', 'sonja', 'hogg', '197475', 'barmore', 'left', 'boy', 'job', 'ruston', 'high', 'become', 'hogg', 'assistant', '1977', 'thinking', 'use', 'job', 'get', 'campus', 'eventually', 'assume', 'men', 'job', 'turned', 'twice', 'men', 'job', 'early', '80', 'turned', 'late', '80', 'late', 'say', 'tech', 'penultimate', 'aiaw', 'title', '1981', 'first', 'ncaa', 'title', '1982', 'barmore', 'named', 'cocoach', 'hogg', '198283', 'sole', 'head', 'coach', '198586', 'barmore', 'tech', 'nine', 'ncaa', 'final', 'four', '1988', 'title', 'worst', 'record', '1812', '199091', 'everyone', 'played', 'four', 'year', 'gone', 'least', 'one', 'final', 'four', '12', 'allamericans', 'six', 'player', 'wnba', 'barmore', 'point', 'school', 'president', 'taylor', 'dan', 'reneau', 'taylor', 'successor', 'reason', 'tech', 'success', '11', 'athletic', 'director', 'could', 'cared', 'less', 'woman', 'basketball', 'barmore', 'say', 'two', 'president', 'never', 'wavered', 'barmore', 'also', 'string', 'outstanding', 'assistant', 'coach', 'including', 'current', 'head', 'coach', 'fortner', 'gary', 'blair', 'arkansas', 'kristy', 'curry', 'purdue', 'assistant', '15', 'year', 'former', 'tech', 'allamerican', 'kim', 'mulkeyrobertson', 'heir', 'apparent', 'took', 'baylor', 'job', 'prompting', 'barmore', 'return', 'reality', 'perspective', 'given', 'support', 'barmore', 'showered', 'resource', 'current', 'recruiting', 'budget', '15000', 'got', 'bump', 'salary', 'five', 'year', 'ago', 'base', '130000', 'top', 'coach', 'make', '250000', '400000', 'sometimes', 'resent', 'yes', 'say', 'resource', 'disparity', 'president', 'making', 'money', 'great', 'faculty', 'making', 'money', 'always', 'teamfirst', 'person', 'value', 'important', 'money', 'keep', 'control', 'perspective', 'leon', 'barmore', 'bigger', 'university', 'team', 'leon', 'barmore', 'bigger', 'family', 'turned', 'tempting', 'job', 'lsu', 'came', 'early', '80', 'comet', 'wanted', '1997', 'chancellor', 'hired', 'four', 'consecutive', 'wnba', 'title', 'later', 'new', 'york', 'liberty', 'wooed', 'came', 'family', 'barmore', 'say', 'wife', 'daughter', 'evaluated', 'happiness', 'three', 'u', 'thought', 'two', 'want', 'go', 'two', 'three', 'win', 'every', 'time', 'wouldarth', 'vader', 'tag', 'lingers', 'smalltown', 'symmetry', 'life', 'two', 'year', 'ago', 'barmore', 'sold', 'home', '28', 'year', 'mile', 'campus', 'assistant', 'coach', 'chris', 'long', 'bought', 'dream', 'home', '3', 'mile', 'campus', 'wife', 'rachel', 'retired', '31', 'year', 'fifthgrade', 'teacher', 'daughter', 'shannon', 'began', 'teaching', 'third', 'grade', 'school', 'barmore', 'like', 'rhythm', 'ruston', 'good', 'golf', 'course', 'less', '15', 'minute', 'need', 'starting', 'time', 'amid', 'contentment', 'concern', 'legacy', 'wonder', 'woman', 'basketball', 'hall', 'fame', 'need', 'chancellor', 'say', 'barmore', 'remembers', 'texas', 'coach', 'jody', 'conradt', 'called', 'darth', 'vader', 'late', '80', 'feel', 'outsider', 'judge', 'game', 'face', 'scowl', 'intimidating', 'given', 'life', 'woman', 'basketball', 'say', 'yet', 'feel', 'like', 'many', 'understand', 'know', 'see', 'tough', 'outer', 'core', 'player', 'assistant', 'friend', 'tell', 'story', 'generosity', 'loyalty', 'lassiter', 'receiving', 'end', 'barmore', 'stare', 'well', 'encouraging', 'note', 'phone', 'call', 'deep', 'inside', 'care', 'player', 'around', 'see', 'driving', 'tour', 'barmore', 'sits', 'cafe', 'recalling', 'country', 'music', 'lyric', 'fried', 'catfish', 'boiled', 'potato', 'corn', 'sweetened', 'iced', 'tea', 'also', 'repeat', 'favorite', 'saying', 'one', 'lock', 'tradition', 'guard', 'gone', 'guarded', 'tradition', '20', 'year', 'favorite', 'school', 'favorite', 'town', 'great', 'story', 'world', 'sport', 'never', 'taken', 'timeout', 'never', 'disappeared', 'say', 'happen', 'illustration', 'photo', 'color', 'bill', 'haber', 'ap', '2', 'photo', 'b', 'w', 'gene', 'j', 'puskar', 'ap', 'caption', 'leon', 'barmoreallaround', 'ayana', 'bird', 'walker', 'average', '135', 'point', '93', 'rebound', 'man', 'varied', 'face', 'leon', 'barmore', 'known', 'scowl', 'also', 'generosity', 'loyalty', 'subject', 'coach', 'manager', 'tournament', 'championship', 'personal', 'profile', 'college', 'basketball', 'louisiana', 'tech', 'university']\",\n          \"['look', 'top', 'five', 'upperechelon', 'player', 'chasing', 'tiger', 'wood', 'lee', 'trevino', 'talented', 'golfer', 'era', 'let', 'stop', 'winning', 'six', 'major', 'reason', 'six', 'outworked', 'jack', 'nicklaus', 'trevino', 'said', 'jack', 'talented', 'could', 'outwork', 'know', 'jack', 'great', 'husband', 'great', 'daddy', 'five', 'kid', 'trevino', 'cost', 'two', 'family', 'trevino', 'said', 'family', 'came', 'second', 'golf', 'always', 'trevino', 'see', 'similarity', 'nicklaus', 'phil', 'mickelson', 'come', 'family', 'life', 'big', 'difference', 'mickelson', 'butt', 'head', 'bachelor', 'named', 'tiger', 'tiger', 'strongest', 'talented', 'player', 'unlike', 'jack', 'also', 'outwork', 'everybody', 'else', 'trevino', 'said', 'phil', 'mickelson', 'player', 'talented', 'tiger', 'wood', 'phil', 'mickelson', 'give', 'golf', '100', 'percent', 'priority', 'important', 'sky', 'limit', 'mickelson', 'last', 'time', 'played', 'hazeltine', 'national', '1991', 'u', 'open', 'mickelson', 'third', 'major', 'coming', 'junior', 'season', 'arizona', 'state', 'finished', '55th', 'turned', '21', 'sunday', 'mickelson', 'one', 'four', 'fourtime', 'firstteam', 'allamericas', 'ncaa', 'golf', 'nicklaus', 'wood', 'player', 'win', 'ncaa', 'title', 'u', 'amateur', 'year', 'three', 'ncaa', 'title', 'overall', 'also', 'last', 'amateur', 'win', 'pga', 'tour', 'event', '1991', 'northern', 'telecom', 'open', 'really', 'thought', 'would', 'winning', 'major', 'championship', 'mid', '90', 'mickelson', 'said', 'seemed', 'destiny', 'yet', 'major', '41', 'attempt', 'return', 'hazeltine', '32', 'mickelson', 'best', 'known', 'best', 'golfer', 'never', 'win', 'major', 'every', 'respect', 'mickelson', 'career', 'success', '21', 'pga', 'tour', 'victory', 'second', 'behind', 'wood', 'among', 'active', 'player', 'also', '21', 'million', '2', 'career', 'earings', 'behind', 'wood', 'total', '30', 'million', 'course', 'ranked', '2', 'world', 'behind', 'know', 'mickelson', 'wood', 'close', 'yet', 'world', 'apart', 'wood', 'single', 'guy', 'hold', 'eight', 'major', 'title', 'remains', 'driven', 'best', 'golfer', 'history', 'mickelson', 'foremost', 'husband', 'amy', 'dad', '3yearold', 'amanda', 'brynn', '10monthold', 'sophia', 'isabel', '1999', 'mickelson', 'vowed', 'would', 'surrender', 'u', 'open', 'battle', 'payne', 'stewart', 'amy', 'went', 'labor', 'first', 'child', 'amanda', 'delayed', 'arrival', '18', 'hour', 'father', 'finished', 'second', 'stewart', 'last', 'fall', 'mickelson', 'skipped', 'seasonending', 'tour', 'championship', 'case', 'colic', 'sophia', '12', 'week', 'old', 'mickelson', 'defending', 'champion', 'ernie', 'el', 'also', 'family', 'man', 'joked', 'british', 'open', 'jesper', 'parnevik', 'thought', 'best', 'way', 'catch', 'wood', 'parnevik', 'introduced', 'wood', 'nanny', 'swedish', 'model', 'elin', 'nordegren', 'still', 'dating', 'tiger', 'probably', 'settle', 'el', 'said', 'smile', 'get', 'married', 'kid', 'mickelson', 'agree', 'family', 'distracts', 'playing', 'best', 'golf', 'great', 'wife', 'family', 'support', 'system', 'want', 'need', 'play', 'best', 'mickelson', 'said', 'traveling', 'wife', 'watch', 'many', 'round', 'allows', 'play', 'best', 'going', 'spend', '365', 'day', 'year', 'golforiented', 'think', 'dimension', 'one', 'taking', 'five', 'month', 'family', 'mickelson', 'returned', 'january', 'first', 'tournament', 'bob', 'hope', 'chrysler', 'classic', 'finished', 'third', 'master', 'second', 'u', 'open', 'wood', 'time', 'mickelson', 'numerous', 'chance', 'win', 'master', 'u', 'open', 'pga', 'championship', 'five', 'top10', 'finish', 'including', 'second', 'place', 'last', 'year', 'pga', 'championship', 'mickelson', 'best', 'finish', 'british', 'open', '11th', '2000', 'maintained', 'positive', 'attitude', 'despite', 'coming', 'short', 'many', 'time', 'claim', 'enjoy', 'chasing', 'wood', 'wonderful', 'challenge', 'play', 'potentially', 'greatest', 'player', 'time', 'trevino', 'predicts', 'mickelson', 'attitude', 'talent', 'ultimately', 'lead', 'winning', 'multiple', 'major', 'phil', 'handle', 'pressure', 'question', 'extremely', 'well', 'trevino', 'said', 'look', 'colin', 'montgomerie', 'handle', 'asked', 'advice', 'mickelson', 'course', 'trevino', 'considers', 'devoted', 'family', 'man', 'two', 'young', 'child', 'laughed', 'hope', 'phil', 'never', 'change', 'trevino', 'said', 'good', 'father', 'husband', 'phil', 'mickelson', 'age', '32', 'turned', 'pro', '1992', 'victory', '22', 'worldwide', 'including', '21', 'pga', 'tour', 'major', 'championship', 'none', 'chasing', 'tiger', 'moment', 'mickelson', 'finished', 'second', 'third', 'third', 'past', 'three', 'major', 'wood', '2002', 'u', 'open', '2002', 'master', '2001', 'master', 'paired', 'wood', 'final', 'round', '2001', 'master', 'tied', 'twice', 'front', 'nine', 'illustration', 'photo', 'photo']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 2002.0,\n        \"max\": 2002.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2002.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a12ae5ef-651d-4ac2-a4b1-d04e1f03d2b9\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3935</td>\n",
       "      <td>said, like, would, one, get, going, time, year...</td>\n",
       "      <td>['14', 'year', 'coaching', '39', 'year', 'livi...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>said, like, would, one, get, going, time, year...</td>\n",
       "      <td>['see', 'sidebar', '02c', 'ruston', 'la', 'din...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3757</td>\n",
       "      <td>said, like, would, one, get, going, time, year...</td>\n",
       "      <td>['gave', 'garden', 'exactly', 'missed', 'lonel...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3426</td>\n",
       "      <td>said, like, would, one, get, going, time, year...</td>\n",
       "      <td>['indianapolis', 'ten', 'year', 'ago', 'chuck'...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5945</td>\n",
       "      <td>u, world, olympic, 2023, championship, united,...</td>\n",
       "      <td>['look', 'top', 'five', 'upperechelon', 'playe...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a12ae5ef-651d-4ac2-a4b1-d04e1f03d2b9')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a12ae5ef-651d-4ac2-a4b1-d04e1f03d2b9 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a12ae5ef-651d-4ac2-a4b1-d04e1f03d2b9');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ddf81777-0b2d-47ec-be59-ebaecf4272a0\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddf81777-0b2d-47ec-be59-ebaecf4272a0')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ddf81777-0b2d-47ec-be59-ebaecf4272a0 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Document_No Dominant_Topic Topic_Perc_Contrib  \\\n",
       "0             0             22             0.3935   \n",
       "1             1             22             0.2309   \n",
       "2             2             22             0.3757   \n",
       "3             3             22             0.3426   \n",
       "46           46             16             0.5945   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   said, like, would, one, get, going, time, year...   \n",
       "1   said, like, would, one, get, going, time, year...   \n",
       "2   said, like, would, one, get, going, time, year...   \n",
       "3   said, like, would, one, get, going, time, year...   \n",
       "46  u, world, olympic, 2023, championship, united,...   \n",
       "\n",
       "                                                 Text    Year  \n",
       "0   ['14', 'year', 'coaching', '39', 'year', 'livi...  2002.0  \n",
       "1   ['see', 'sidebar', '02c', 'ruston', 'la', 'din...  2002.0  \n",
       "2   ['gave', 'garden', 'exactly', 'missed', 'lonel...  2002.0  \n",
       "3   ['indianapolis', 'ten', 'year', 'ago', 'chuck'...  2002.0  \n",
       "46  ['look', 'top', 'five', 'upperechelon', 'playe...  2002.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing texts for each year...\n",
      "Processing documents for year: 2002.0\n",
      "Processed 9158 documents for year 2002.0.\n",
      "Processing documents for year: 2003.0\n",
      "Processed 10338 documents for year 2003.0.\n",
      "Processing documents for year: 2004.0\n",
      "Processed 9014 documents for year 2004.0.\n",
      "Processing documents for year: 2006.0\n",
      "Processed 8764 documents for year 2006.0.\n",
      "Processing documents for year: 2007.0\n",
      "Processed 7396 documents for year 2007.0.\n",
      "Processing documents for year: 2009.0\n",
      "Processed 6762 documents for year 2009.0.\n",
      "Processing documents for year: 2010.0\n",
      "Processed 6744 documents for year 2010.0.\n",
      "Processing documents for year: 2011.0\n",
      "Processed 6111 documents for year 2011.0.\n",
      "Processing documents for year: 2012.0\n",
      "Processed 6461 documents for year 2012.0.\n",
      "Processing documents for year: 2013.0\n",
      "Processed 5534 documents for year 2013.0.\n",
      "Processing documents for year: 2016.0\n",
      "Processed 5377 documents for year 2016.0.\n",
      "Processing documents for year: 2017.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "import os\n",
    "\n",
    "# Define paths to the saved files\n",
    "model_path = '/content/drive/MyDrive/Colab Notebooks/alex/models/lda_30_topics/lda_model_30_topics.gensim'\n",
    "corpus_path = '/content/drive/MyDrive/Colab Notebooks/alex/models/dictionary_and_corpus/corpus.mm'\n",
    "excel_path = '/content/drive/MyDrive/Colab Notebooks/alex/tokenized_newspaper_data.xlsx'\n",
    "\n",
    "# Load the LDA model\n",
    "loaded_lda_model = None\n",
    "try:\n",
    "    loaded_lda_model = LdaModel.load(model_path)\n",
    "    print(f\"LDA model loaded successfully from {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: LDA model file not found at {model_path}. Cannot proceed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the LDA model: {e}\")\n",
    "\n",
    "# Load the corpus\n",
    "corpus = None\n",
    "try:\n",
    "    corpus = corpora.MmCorpus(corpus_path)\n",
    "    print(f\"Corpus loaded successfully from {corpus_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Corpus file not found at {corpus_path}. Cannot proceed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the corpus: {e}\")\n",
    "\n",
    "# Load the tokenized data from Excel\n",
    "tokenized_data_df = None\n",
    "texts = None\n",
    "try:\n",
    "    tokenized_data_df = pd.read_excel(excel_path)\n",
    "    print(f\"Excel file loaded successfully from {excel_path}\")\n",
    "    if 'Main_text_tokenized' in tokenized_data_df.columns:\n",
    "        texts = tokenized_data_df['Main_text_tokenized'].tolist()\n",
    "        print(f\"Extracted {len(texts)} documents for texts variable.\")\n",
    "    else:\n",
    "        print(\"Could not find 'Main_text_tokenized' column in the Excel file. Cannot proceed.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Tokenized data Excel file not found at {excel_path}. Cannot proceed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
    "\n",
    "# Function to find the dominant topic in each sentence (reused from previous work)\n",
    "def format_topics_sentences(ldamodel, corpus, texts=None):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = pd.concat([sent_topics_df, pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]).to_frame().T], ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "     # Add original text to the end of the output\n",
    "    if texts is not None and len(texts) == len(sent_topics_df):\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    elif texts is not None:\n",
    "         print(f\"Warning: Length of texts ({len(texts)}) does not match length of dominant topics ({len(sent_topics_df)}). Text column will not be added.\")\n",
    "\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Create df_dominant_topic and add Year column if data is loaded\n",
    "df_dominant_topic = None\n",
    "if loaded_lda_model is not None and corpus is not None and texts is not None and tokenized_data_df is not None:\n",
    "    try:\n",
    "        df_topic_sents_keywords = format_topics_sentences(ldamodel=loaded_lda_model, corpus=corpus, texts=texts)\n",
    "\n",
    "        if not df_topic_sents_keywords.empty:\n",
    "            df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "            df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "            print(\"\\n'df_dominant_topic' DataFrame created.\")\n",
    "\n",
    "            if 'date' in tokenized_data_df.columns and len(tokenized_data_df) == len(df_dominant_topic):\n",
    "                try:\n",
    "                    dates = pd.to_datetime(tokenized_data_df['date'], errors='coerce')\n",
    "                    years = dates.dt.year.apply(lambda x: int(x) if pd.notna(x) else None).tolist()\n",
    "\n",
    "                    if len(years) == len(df_dominant_topic):\n",
    "                        df_dominant_topic['Year'] = years\n",
    "                        print(\"Year column added to df_dominant_topic from 'date' column.\")\n",
    "                        # display(df_dominant_topic.head()) # Optional: display head after adding year\n",
    "                    else:\n",
    "                        print(f\"Mismatch in number of documents: {len(years)} years extracted vs {len(df_dominant_topic)} in df_dominant_topic. Year column not added.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while extracting years from 'date' column: {e}. Year column not added.\")\n",
    "            else:\n",
    "                print(\"'date' column not found in tokenized_data_df or length mismatch. Cannot add Year column.\")\n",
    "        else:\n",
    "            print(\"'df_topic_sents_keywords' is empty. Cannot create 'df_dominant_topic'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating or processing 'df_dominant_topic': {e}.\")\n",
    "else:\n",
    "    print(\"Cannot create 'df_dominant_topic' as necessary components are not available.\")\n",
    "\n",
    "\n",
    "# Create yearly_dataframes if df_dominant_topic was created successfully with Year\n",
    "yearly_dataframes = None\n",
    "if df_dominant_topic is not None and 'Year' in df_dominant_topic.columns:\n",
    "    print(\"\\nProceeding with filtering by year.\")\n",
    "    unique_years = df_dominant_topic['Year'].dropna().unique()\n",
    "    print(f\"Unique years found: {unique_years}\")\n",
    "\n",
    "    yearly_dataframes = {}\n",
    "    for year in unique_years:\n",
    "        yearly_df = df_dominant_topic[df_dominant_topic['Year'] == year].copy()\n",
    "        yearly_dataframes[year] = yearly_df\n",
    "        print(f\"Filtered dataframe for year {year} with {len(yearly_df)} documents.\")\n",
    "\n",
    "    if yearly_dataframes:\n",
    "        first_year = list(yearly_dataframes.keys())[0]\n",
    "        print(f\"\\nDisplaying head of dataframe for year {first_year}:\")\n",
    "        display(yearly_dataframes[first_year].head())\n",
    "    else:\n",
    "        print(\"No yearly dataframes created.\")\n",
    "else:\n",
    "    print(\"\\nSkipping filtering by year as df_dominant_topic with Year column is not available.\")\n",
    "\n",
    "\n",
    "# Prepare yearly processed texts if yearly_dataframes are available\n",
    "yearly_processed_texts = {}\n",
    "if yearly_dataframes:\n",
    "    print(\"\\nProcessing texts for each year...\")\n",
    "    for year, df_year in yearly_dataframes.items():\n",
    "        print(f\"Processing documents for year: {year}\")\n",
    "        texts_for_year = df_year['Text'].tolist()\n",
    "        processed_texts = []\n",
    "        for text_list_str in texts_for_year:\n",
    "            if isinstance(text_list_str, str):\n",
    "                try:\n",
    "                    text_list = eval(text_list_str)\n",
    "                    if isinstance(text_list, list):\n",
    "                        processed_texts.append([str(token) for token in text_list if isinstance(token, (str, int, float))])\n",
    "                    else:\n",
    "                        processed_texts.append([])\n",
    "                except:\n",
    "                    processed_texts.append([])\n",
    "            else:\n",
    "                processed_texts.append([])\n",
    "        yearly_processed_texts[year] = processed_texts\n",
    "        print(f\"Processed {len(processed_texts)} documents for year {year}.\")\n",
    "    print(\"Finished processing texts for all years.\")\n",
    "else:\n",
    "    print(\"Skipping processing texts as yearly_dataframes are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d13d5daa"
   },
   "source": [
    "## 2. Get the coherence of each topic by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6d22cd9"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate the semantic coherence for each topic for each year using the `CoherenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7dc87cb"
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Initialize dictionary to store yearly topic coherence scores\n",
    "yearly_topic_coherence = {}\n",
    "\n",
    "# Check if yearly_processed_texts, model, and dictionary are loaded\n",
    "if yearly_processed_texts and loaded_lda_model is not None and loaded_lda_model.id2word is not None:\n",
    "    print(\"Calculating topic coherence for each year...\")\n",
    "    for year, processed_texts_for_year in yearly_processed_texts.items():\n",
    "        print(f\"Calculating topic coherence for year: {year}\")\n",
    "        if processed_texts_for_year:\n",
    "            try:\n",
    "                # Create a CoherenceModel instance\n",
    "                coherence_model_lda_year = CoherenceModel(model=loaded_lda_model, texts=processed_texts_for_year, dictionary=loaded_lda_model.id2word, coherence='c_v')\n",
    "\n",
    "                # Calculate the coherence scores for each topic for the current year's data\n",
    "                topic_coherence_lda_year = coherence_model_lda_year.get_coherence_per_topic()\n",
    "\n",
    "                # Store the calculated topic coherence scores\n",
    "                yearly_topic_coherence[year] = topic_coherence_lda_year\n",
    "                print(f\"Calculated topic coherence for year {year}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating topic coherence for year {year}: {e}\")\n",
    "                yearly_topic_coherence[year] = None # Store None if calculation fails\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping topic coherence calculation for year {year} due to empty processed texts.\")\n",
    "            yearly_topic_coherence[year] = None # Store None if skipping\n",
    "\n",
    "    print(\"\\nFinished calculating yearly topic coherence scores.\")\n",
    "\n",
    "    # Store and display the results in a DataFrame\n",
    "    if yearly_topic_coherence:\n",
    "        print(\"Storing and displaying yearly topic coherence scores...\")\n",
    "        yearly_coherence_series = pd.Series(yearly_topic_coherence)\n",
    "        yearly_topic_coherence_df = pd.DataFrame(yearly_coherence_series.tolist(), index=yearly_coherence_series.index)\n",
    "        yearly_topic_coherence_df.columns = [f'Topic {i}' for i in range(yearly_topic_coherence_df.shape[1])]\n",
    "        yearly_topic_coherence_df = yearly_topic_coherence_df.sort_index()\n",
    "\n",
    "        print(\"\\nYearly Topic Coherence Scores:\")\n",
    "        display(yearly_topic_coherence_df)\n",
    "\n",
    "        print(\"\\nYearly topic coherence scores stored and displayed successfully.\")\n",
    "    else:\n",
    "        print(\"No yearly topic coherence scores were calculated.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"'yearly_processed_texts' is not available or is empty, or model/dictionary missing. Cannot calculate yearly topic coherence scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e45c244e"
   },
   "source": [
    "## 3. Get the exclusivity of each topic by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef724943"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate the semantic exclusivity for each topic for each year using a custom metric based on topic-word distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fccebe4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize dictionary to store yearly topic exclusivity scores\n",
    "yearly_topic_exclusivity = {}\n",
    "\n",
    "# Check if yearly_processed_texts and loaded_lda_model are available\n",
    "if yearly_processed_texts and loaded_lda_model is not None:\n",
    "    print(\"Calculating topic exclusivity for each year...\")\n",
    "    # Get the topic-word distributions from the loaded model\n",
    "    topic_word_distributions = loaded_lda_model.get_topics() # Shape (num_topics, vocab_size)\n",
    "    num_topics = topic_word_distributions.shape[0]\n",
    "    vocab_size = topic_word_distributions.shape[1]\n",
    "\n",
    "    if num_topics > 1:\n",
    "         # Iterate through the yearly_processed_texts dictionary\n",
    "        for year, processed_texts_for_year in yearly_processed_texts.items():\n",
    "            print(f\"Calculating topic exclusivity for year: {year}\")\n",
    "            # Check if the processed texts for the current year are not empty\n",
    "            if processed_texts_for_year:\n",
    "                try:\n",
    "                    # Calculate exclusivity for each topic for the current year\n",
    "                    topic_exclusivity_scores_year = []\n",
    "                    for topic_id in range(num_topics):\n",
    "                        topic_word_dist = topic_word_distributions[topic_id, :] # p(w|z) for the current topic\n",
    "\n",
    "                        # Calculate the probability of word w in any topic other than z: sum_{z' != z} p(w|z')\n",
    "                        sum_p_w_other_topics = np.sum(np.delete(topic_word_distributions, topic_id, axis=0), axis=0)\n",
    "\n",
    "                        # Using a simple metric based on p(w|z) * (p(w|z) - max_{z' != z} p(w|z'))\n",
    "                        # Consider top words of the topic\n",
    "                        top_word_indices = np.argsort(topic_word_dist)[::-1][:20] # Consider top 20 words\n",
    "\n",
    "                        topic_exclusivity_score = 0\n",
    "                        for word_idx in top_word_indices:\n",
    "                            p_w_z = topic_word_dist[word_idx]\n",
    "                            max_p_w_not_z = np.max(np.delete(topic_word_distributions, topic_id, axis=0)[:, word_idx])\n",
    "\n",
    "                            word_exclusivity = p_w_z * (p_w_z - max_p_w_not_z)\n",
    "                            topic_exclusivity_score += word_exclusivity\n",
    "\n",
    "                        topic_exclusivity_scores_year.append(topic_exclusivity_score)\n",
    "\n",
    "                    yearly_topic_exclusivity[year] = topic_exclusivity_scores_year\n",
    "                    print(f\"Calculated topic exclusivity for year {year}.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while calculating topic exclusivity for year {year}: {e}\")\n",
    "                    yearly_topic_exclusivity[year] = None # Store None if calculation fails\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping topic exclusivity calculation for year {year} due to empty processed texts.\")\n",
    "                yearly_topic_exclusivity[year] = None # Store None if skipping\n",
    "        print(\"\\nFinished calculating yearly topic exclusivity scores.\")\n",
    "\n",
    "        # Store and display the results in a DataFrame\n",
    "        if yearly_topic_exclusivity:\n",
    "            print(\"Storing and displaying yearly topic exclusivity scores...\")\n",
    "            yearly_exclusivity_series = pd.Series(yearly_topic_exclusivity)\n",
    "            yearly_topic_exclusivity_df = pd.DataFrame(yearly_exclusivity_series.tolist(), index=yearly_exclusivity_series.index)\n",
    "            yearly_topic_exclusivity_df.columns = [f'Topic {i}' for i in range(yearly_topic_exclusivity_df.shape[1])]\n",
    "            yearly_topic_exclusivity_df = yearly_topic_exclusivity_df.sort_index()\n",
    "\n",
    "            print(\"\\nYearly Topic Exclusivity Scores:\")\n",
    "            display(yearly_topic_exclusivity_df)\n",
    "\n",
    "            print(\"\\nYearly topic exclusivity scores stored and displayed successfully.\")\n",
    "        else:\n",
    "             print(\"No yearly topic exclusivity scores were calculated.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping exclusivity calculation as there is only one topic.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"'yearly_processed_texts' is not available or is empty, or loaded_lda_model is missing. Cannot calculate yearly topic exclusivity scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "453fc284"
   },
   "source": [
    "## 4. Visualize the coherences for each topic per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aaa4635"
   },
   "source": [
    "**Reasoning**:\n",
    "Create line plots for each topic showing its semantic coherence score over the years using the stored yearly topic coherence DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "862095d1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if yearly_topic_coherence_df is available and not empty\n",
    "if 'yearly_topic_coherence_df' in locals() and not yearly_topic_coherence_df.empty:\n",
    "    print(\"Generating line plots for each topic's coherence over time...\")\n",
    "\n",
    "    # Get the list of topic columns\n",
    "    topic_columns = [col for col in yearly_topic_coherence_df.columns if col.startswith('Topic')]\n",
    "\n",
    "    # Iterate through each topic column and create a plot\n",
    "    for topic_col in topic_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Plot the coherence score for the current topic against the years (index)\n",
    "        plt.plot(yearly_topic_coherence_df.index, yearly_topic_coherence_df[topic_col], marker='o', linestyle='-')\n",
    "\n",
    "        # Add title and axis labels\n",
    "        plt.title(f'Semantic Coherence (c_v) for {topic_col} Over Time')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Coherence Score')\n",
    "\n",
    "        # Add grid for better readability\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nFinished generating topic coherence plots.\")\n",
    "\n",
    "else:\n",
    "    print(\"'yearly_topic_coherence_df' DataFrame not found or is empty. Cannot generate plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b930492f"
   },
   "source": [
    "## 5. Visualize the exclusivity for each topic per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e96c96f8"
   },
   "source": [
    "**Reasoning**:\n",
    "Create line plots for each topic showing its semantic exclusivity score over the years using the stored yearly topic exclusivity DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dd9c248"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if yearly_topic_exclusivity_df is available and not empty\n",
    "if 'yearly_topic_exclusivity_df' in locals() and not yearly_topic_exclusivity_df.empty:\n",
    "    print(\"Generating line plots for each topic's exclusivity over time...\")\n",
    "\n",
    "    # Get the list of topic columns\n",
    "    topic_columns = [col for col in yearly_topic_exclusivity_df.columns if col.startswith('Topic')]\n",
    "\n",
    "    # Iterate through each topic column and create a plot\n",
    "    for topic_col in topic_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Plot the exclusivity score for the current topic against the years (index)\n",
    "        plt.plot(yearly_topic_exclusivity_df.index, yearly_topic_exclusivity_df[topic_col], marker='o', linestyle='-')\n",
    "\n",
    "        # Add title and axis labels\n",
    "        plt.title(f'Semantic Exclusivity for {topic_col} Over Time')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Exclusivity Score')\n",
    "\n",
    "        # Add grid for better readability\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nFinished generating topic exclusivity plots.\")\n",
    "\n",
    "else:\n",
    "    print(\"'yearly_topic_exclusivity_df' DataFrame not found or is empty. Cannot generate plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb5b418f"
   },
   "source": [
    "## 6. Save the data and models in Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2d7b2d1"
   },
   "source": [
    "**Reasoning**:\n",
    "Save the generated dataframes (yearly topic coherence and exclusivity) to Excel files and the dictionaries (yearly dataframes, yearly coherence, yearly exclusivity) to pickle files for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d736fbe3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory to save the files in your Google Drive\n",
    "save_dir = '/content/drive/MyDrive/Colab Notebooks/alex/saved_analysis_data/30_topics_model'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Save yearly_topic_coherence_df DataFrame to Excel ---\n",
    "excel_coherence_path = os.path.join(save_dir, 'yearly_30_topics_coherence_scores.xlsx')\n",
    "if 'yearly_topic_coherence_df' in locals() and yearly_topic_coherence_df is not None and not yearly_topic_coherence_df.empty:\n",
    "    print(f\"Saving yearly topic coherence scores to {excel_coherence_path}...\")\n",
    "    try:\n",
    "        yearly_topic_coherence_df.to_excel(excel_coherence_path, index=True)\n",
    "        print(\"Yearly topic coherence scores saved successfully to Excel.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly topic coherence scores to Excel: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_topic_coherence_df' DataFrame not found or is empty. Skipping save.\")\n",
    "\n",
    "\n",
    "# --- Save yearly_topic_exclusivity_df DataFrame to Excel ---\n",
    "excel_exclusivity_path = os.path.join(save_dir, 'yearly_30_topics_exclusivity_scores.xlsx')\n",
    "if 'yearly_topic_exclusivity_df' in locals() and yearly_topic_exclusivity_df is not None and not yearly_topic_exclusivity_df.empty:\n",
    "    print(f\"Saving yearly topic exclusivity scores to {excel_exclusivity_path}...\")\n",
    "    try:\n",
    "        yearly_topic_exclusivity_df.to_excel(excel_exclusivity_path, index=True)\n",
    "        print(\"Yearly topic exclusivity scores saved successfully to Excel.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly topic exclusivity scores to Excel: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_topic_exclusivity_df' DataFrame not found or is empty. Skipping save.\")\n",
    "\n",
    "\n",
    "# --- Save yearly_dataframes dictionary using pickle ---\n",
    "yearly_dataframes_path = os.path.join(save_dir, 'yearly_30_topics_dataframes.pkl')\n",
    "if 'yearly_dataframes' in locals() and yearly_dataframes:\n",
    "    print(f\"Saving yearly_dataframes to {yearly_dataframes_path}...\")\n",
    "    try:\n",
    "        with open(yearly_dataframes_path, 'wb') as f:\n",
    "            pickle.dump(yearly_dataframes, f)\n",
    "        print(\"yearly_dataframes saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly_dataframes: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_dataframes' dictionary not found. Skipping save.\")\n",
    "\n",
    "\n",
    "# --- Save yearly_coherence_scores dictionary using pickle ---\n",
    "# Note: This saves the overall yearly coherence scores, not topic-specific ones per year\n",
    "yearly_coherence_scores_path = os.path.join(save_dir, 'yearly_30_topics_coherence_scores.pkl')\n",
    "if 'yearly_coherence_scores' in locals() and yearly_coherence_scores:\n",
    "    print(f\"Saving yearly_coherence_scores to {yearly_coherence_scores_path}...\")\n",
    "    try:\n",
    "        with open(yearly_coherence_scores_path, 'wb') as f:\n",
    "            pickle.dump(yearly_coherence_scores, f)\n",
    "        print(\"yearly_coherence_scores saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly_coherence_scores: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_coherence_scores' dictionary not found. Skipping save.\")\n",
    "\n",
    "\n",
    "# --- Save yearly_topic_coherence dictionary using pickle ---\n",
    "# This saves the dictionary containing list of topic coherences per year\n",
    "yearly_topic_coherence_dict_path = os.path.join(save_dir, 'yearly_30_topics_coherence_dict.pkl')\n",
    "if 'yearly_topic_coherence' in locals() and yearly_topic_coherence:\n",
    "    print(f\"Saving yearly_topic_coherence dictionary to {yearly_topic_coherence_dict_path}...\")\n",
    "    try:\n",
    "        with open(yearly_topic_coherence_dict_path, 'wb') as f:\n",
    "            pickle.dump(yearly_topic_coherence, f)\n",
    "        print(\"yearly_topic_coherence dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly_topic_coherence dictionary: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_topic_coherence' dictionary not found. Skipping save.\")\n",
    "\n",
    "\n",
    "# --- Save yearly_topic_exclusivity dictionary using pickle ---\n",
    "# This saves the dictionary containing list of topic exclusivities per year\n",
    "yearly_topic_exclusivity_dict_path = os.path.join(save_dir, 'yearly_30_topics_exclusivity_dict.pkl')\n",
    "if 'yearly_topic_exclusivity' in locals() and yearly_topic_exclusivity:\n",
    "    print(f\"Saving yearly_topic_exclusivity dictionary to {yearly_topic_exclusivity_dict_path}...\")\n",
    "    try:\n",
    "        with open(yearly_topic_exclusivity_dict_path, 'wb') as f:\n",
    "            pickle.dump(yearly_topic_exclusivity, f)\n",
    "        print(\"yearly_topic_exclusivity dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving yearly_topic_exclusivity dictionary: {e}\")\n",
    "else:\n",
    "    print(\"'yearly_topic_exclusivity' dictionary not found. Skipping save.\")\n",
    "\n",
    "\n",
    "print(\"\\nSave process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "577232dd"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The notebook has been reorganized to follow the requested structure. The code cells for loading data, calculating and visualizing yearly topic coherence and exclusivity, and saving the results have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6iA_x891Uhe"
   },
   "outputs": [],
   "source": [
    "%pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0HzoIsXMJ9I"
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib import colors\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory to save temporary images and CSVs for the PDF\n",
    "temp_dir = '/content/temp_pdf_assets'\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# --- Save Plots as Images ---\n",
    "image_paths = []\n",
    "if 'yearly_topic_coherence_df' in locals() and not yearly_topic_coherence_df.empty:\n",
    "    print(\"Saving coherence plots as images...\")\n",
    "    topic_columns = [col for col in yearly_topic_coherence_df.columns if col.startswith('Topic')]\n",
    "    for topic_col in topic_columns:\n",
    "        plt.figure(figsize=(12, 6)) # Increased figure size\n",
    "        plt.plot(yearly_topic_coherence_df.index, yearly_topic_coherence_df[topic_col], marker='o', linestyle='-')\n",
    "        plt.title(f'Semantic Coherence (c_v) for {topic_col} Over Time')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Coherence Score')\n",
    "        plt.grid(True)\n",
    "        img_path = os.path.join(temp_dir, f'{topic_col}_coherence.png')\n",
    "        plt.savefig(img_path)\n",
    "        image_paths.append(img_path)\n",
    "        plt.close() # Close the plot to free up memory\n",
    "    print(\"Finished saving coherence plots.\")\n",
    "\n",
    "if 'yearly_topic_exclusivity_df' in locals() and not yearly_topic_exclusivity_df.empty:\n",
    "    print(\"Saving exclusivity plots as images...\")\n",
    "    topic_columns = [col for col in yearly_topic_exclusivity_df.columns if col.startswith('Topic')]\n",
    "    for topic_col in topic_columns:\n",
    "        plt.figure(figsize=(12, 6)) # Increased figure size\n",
    "        plt.plot(yearly_topic_exclusivity_df.index, yearly_topic_exclusivity_df[topic_col], marker='o', linestyle='-')\n",
    "        plt.title(f'Semantic Exclusivity for {topic_col} Over Time')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Exclusivity Score')\n",
    "        plt.grid(True)\n",
    "        img_path = os.path.join(temp_dir, f'{topic_col}_exclusivity.png')\n",
    "        plt.savefig(img_path)\n",
    "        image_paths.append(img_path)\n",
    "        plt.close() # Close the plot to free up memory\n",
    "    print(\"Finished saving exclusivity plots.\")\n",
    "\n",
    "\n",
    "# --- Save DataFrames as CSV for Table Inclusion ---\n",
    "# Skipping CSV save as requested by the user\n",
    "# coherence_csv_path = os.path.join(temp_dir, 'yearly_coherence_scores.csv')\n",
    "# if 'yearly_topic_coherence_df' in locals() and yearly_topic_coherence_df is not None and not yearly_topic_coherence_df.empty:\n",
    "#     yearly_topic_coherence_df.to_csv(coherence_csv_path, index=True)\n",
    "#     print(f\"Yearly topic coherence scores saved to {coherence_csv_path}\")\n",
    "# else:\n",
    "#     coherence_csv_path = None\n",
    "#     print(\"'yearly_topic_coherence_df' DataFrame not found or is empty. Skipping CSV save for coherence.\")\n",
    "\n",
    "# exclusivity_csv_path = os.path.join(temp_dir, 'yearly_exclusivity_scores.csv')\n",
    "# if 'yearly_topic_exclusivity_df' in locals() and yearly_topic_exclusivity_df is not None and not yearly_topic_exclusivity_df.empty:\n",
    "#     yearly_topic_exclusivity_df.to_csv(exclusivity_csv_path, index=True)\n",
    "#     print(f\"Yearly topic exclusivity scores saved to {exclusivity_csv_path}\")\n",
    "# else:\n",
    "#     exclusivity_csv_path = None\n",
    "#     print(\"'yearly_topic_exclusivity_df' DataFrame not found or is empty. Skipping CSV save for exclusivity.\")\n",
    "\n",
    "\n",
    "# --- Create PDF Report ---\n",
    "pdf_path = '/content/drive/MyDrive/Colab Notebooks/alex/saved_analysis_data/30_topics_model/topic_analysis_report_charts_only_30_topics.pdf' # Changed filename\n",
    "doc = SimpleDocTemplate(pdf_path, pagesize=letter) # Use portrait pagesize for charts only\n",
    "story = []\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(name='TableText', fontSize=8)) # Keep TableText style just in case, though not used for tables now\n",
    "\n",
    "# Add Title\n",
    "story.append(Paragraph(\"Topic Analysis Report (20 Topics Model) - Charts Only\", styles['Title']))\n",
    "story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "# Add Plots\n",
    "story.append(Paragraph(\"Yearly Topic Coherence and Exclusivity Plots\", styles['Heading2']))\n",
    "story.append(Spacer(1, 0.1*inch))\n",
    "\n",
    "for img_path in image_paths:\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image(img_path, width=6*inch, height=3*inch) # Increased image size\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.1*inch))\n",
    "\n",
    "try:\n",
    "    doc.build(story)\n",
    "    print(f\"\\nPDF report generated successfully at {pdf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating PDF report: {e}\")\n",
    "\n",
    "# Optional: Clean up temporary files\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "# print(\"Cleaned up temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **© 2024–2025 MD Rafsun Sheikh**\n",
    "##### **Licensed under the Apache License, Version 2.0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0eQAbW19VkR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7B4Deu8TmIXodKzjSjLfd",
   "provenance": [
    {
     "file_id": "1BWKpgg_hUg686qb_tdLiXvL7hrYccliU",
     "timestamp": 1756640712679
    },
    {
     "file_id": "18S9fS7LBzk_QbvKDBBgcpfkSaxC60p4l",
     "timestamp": 1756628041576
    },
    {
     "file_id": "1ZGcRMPq9ISUdHDLxgbsAXAsZE6g1zwDv",
     "timestamp": 1756627873760
    },
    {
     "file_id": "1GLVxVUWGInr2SJz8phf8z9q4zKIWBnH5",
     "timestamp": 1756540971471
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

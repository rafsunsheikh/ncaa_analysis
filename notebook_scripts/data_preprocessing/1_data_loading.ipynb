{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848cba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "folder_path = \"../Data/NCAA_Newspaper_articles\"\n",
    "\n",
    "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7e89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for file_path in txt_files:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            all_texts.append({\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'content': content\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8d5012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ______________________________________________...\n",
       "1      ______________________________________________...\n",
       "2      ______________________________________________...\n",
       "3      ______________________________________________...\n",
       "4      ______________________________________________...\n",
       "                             ...                        \n",
       "186    ______________________________________________...\n",
       "187    ______________________________________________...\n",
       "188    ______________________________________________...\n",
       "189    ______________________________________________...\n",
       "190    ______________________________________________...\n",
       "Name: content, Length: 191, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f55227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191 text files to process...\n",
      "Processing 2002 1.txt...\n",
      "Processing 2002 10.txt...\n",
      "Processing 2002 11.txt...\n",
      "Processing 2002 2.txt...\n",
      "Processing 2002 3.txt...\n",
      "Processing 2002 4.txt...\n",
      "Processing 2002 5.txt...\n",
      "Processing 2002 6.txt...\n",
      "Processing 2002 7.txt...\n",
      "Processing 2002 8.txt...\n",
      "Processing 2002 9.txt...\n",
      "Processing 2003 1.txt...\n",
      "Processing 2003 10.txt...\n",
      "Processing 2003 11.txt...\n",
      "Processing 2003 12.txt...\n",
      "Processing 2003 13.txt...\n",
      "Processing 2003 2.txt...\n",
      "Processing 2003 3.txt...\n",
      "Processing 2003 4.txt...\n",
      "Processing 2003 5.txt...\n",
      "Processing 2003 6.txt...\n",
      "Processing 2003 7.txt...\n",
      "Processing 2003 8.txt...\n",
      "Processing 2003 9.txt...\n",
      "Processing 2004 1.txt...\n",
      "Processing 2004 10.txt...\n",
      "Processing 2004 11.txt...\n",
      "Processing 2004 12.txt...\n",
      "Processing 2004 2.txt...\n",
      "Processing 2004 3.txt...\n",
      "Processing 2004 4.txt...\n",
      "Processing 2004 5.txt...\n",
      "Processing 2004 6.txt...\n",
      "Processing 2004 7 .txt...\n",
      "Processing 2004 8.txt...\n",
      "Processing 2004 9.txt...\n",
      "Processing 2005 1.txt...\n",
      "Processing 2005 10.txt...\n",
      "Processing 2005 11.txt...\n",
      "Processing 2005 12.txt...\n",
      "Processing 2005 2.txt...\n",
      "Processing 2005 3.txt...\n",
      "Processing 2005 4.txt...\n",
      "Processing 2005 5.txt...\n",
      "Processing 2005 6.txt...\n",
      "Processing 2005 7.txt...\n",
      "Processing 2005 8.txt...\n",
      "Processing 2005 9.txt...\n",
      "Processing 2006 1.txt...\n",
      "Processing 2006 10.txt...\n",
      "Processing 2006 2.txt...\n",
      "Processing 2006 3.txt...\n",
      "Processing 2006 4.txt...\n",
      "Processing 2006 5.txt...\n",
      "Processing 2006 6.txt...\n",
      "Processing 2006 7 .txt...\n",
      "Processing 2006 8 .txt...\n",
      "Processing 2006 9 .txt...\n",
      "Processing 2007 1.txt...\n",
      "Processing 2007 10.txt...\n",
      "Processing 2007 2.txt...\n",
      "Processing 2007 3.txt...\n",
      "Processing 2007 4.txt...\n",
      "Processing 2007 5.txt...\n",
      "Processing 2007 6 .txt...\n",
      "Processing 2007 8 .txt...\n",
      "Processing 2007 9 .txt...\n",
      "Processing 2008 1.txt...\n",
      "Processing 2008 2.txt...\n",
      "Processing 2008 3.txt...\n",
      "Processing 2008 4.txt...\n",
      "Processing 2008 5.txt...\n",
      "Processing 2008 6.txt...\n",
      "Processing 2008 7.txt...\n",
      "Processing 2008 8.txt...\n",
      "Processing 2008 9.txt...\n",
      "Processing 2009 1.txt...\n",
      "Processing 2009 2.txt...\n",
      "Processing 2009 3.txt...\n",
      "Processing 2009 4.txt...\n",
      "Processing 2009 5.txt...\n",
      "Processing 2009 6.txt...\n",
      "Processing 2009 7.txt...\n",
      "Processing 2009 8.txt...\n",
      "Processing 2009 9.txt...\n",
      "Processing 2010 1.txt...\n",
      "Processing 2010 2.txt...\n",
      "Processing 2010 3.txt...\n",
      "Processing 2010 4.txt...\n",
      "Processing 2010 5.txt...\n",
      "Processing 2010 6.txt...\n",
      "Processing 2010 7.txt...\n",
      "Processing 2010 8.txt...\n",
      "Processing 2011 1.txt...\n",
      "Processing 2011 2.txt...\n",
      "Processing 2011 3.txt...\n",
      "Processing 2011 4.txt...\n",
      "Processing 2011 5.txt...\n",
      "Processing 2011 6.txt...\n",
      "Processing 2011 7 .txt...\n",
      "Processing 2011 8.txt...\n",
      "Processing 2012 1.txt...\n",
      "Processing 2012 2.txt...\n",
      "Processing 2012 3.txt...\n",
      "Processing 2012 4.txt...\n",
      "Processing 2012 5.txt...\n",
      "Processing 2012 6.txt...\n",
      "Processing 2012 7.txt...\n",
      "Processing 2012 8.txt...\n",
      "Processing 2013 1.txt...\n",
      "Processing 2013 2.txt...\n",
      "Processing 2013 3.txt...\n",
      "Processing 2013 4.txt...\n",
      "Processing 2013 5.txt...\n",
      "Processing 2013 6.txt...\n",
      "Processing 2013 7.txt...\n",
      "Processing 2014 1.txt...\n",
      "Processing 2014 2.txt...\n",
      "Processing 2014 3.txt...\n",
      "Processing 2014 4.txt...\n",
      "Processing 2014 5.txt...\n",
      "Processing 2014 6.txt...\n",
      "Processing 2014 7.txt...\n",
      "Processing 2015 1.txt...\n",
      "Processing 2015 2.txt...\n",
      "Processing 2015 3.txt...\n",
      "Processing 2015 4.txt...\n",
      "Processing 2015 5.txt...\n",
      "Processing 2015 6.txt...\n",
      "Processing 2015 7.txt...\n",
      "Processing 2016 1.txt...\n",
      "Processing 2016 2.txt...\n",
      "Processing 2016 3.txt...\n",
      "Processing 2016 4.txt...\n",
      "Processing 2016 5.txt...\n",
      "Processing 2016 6.txt...\n",
      "Processing 2016 7.txt...\n",
      "Processing 2017 1.txt...\n",
      "Processing 2017 2.txt...\n",
      "Processing 2017 3.txt...\n",
      "Processing 2017 4.txt...\n",
      "Processing 2017 5.txt...\n",
      "Processing 2017 6.txt...\n",
      "Processing 2018 1.txt...\n",
      "Processing 2018 2.txt...\n",
      "Processing 2018 3.txt...\n",
      "Processing 2018 4.txt...\n",
      "Processing 2018 5.txt...\n",
      "Processing 2018 6.txt...\n",
      "Processing 2019 1.txt...\n",
      "Processing 2019 2.txt...\n",
      "Processing 2019 3.txt...\n",
      "Processing 2019 4.txt...\n",
      "Processing 2019 5.txt...\n",
      "Processing 2019 6.txt...\n",
      "Processing 2019 7.txt...\n",
      "Processing 2020 1.txt...\n",
      "Processing 2020 2.txt...\n",
      "Processing 2020 3.txt...\n",
      "Processing 2020 4.txt...\n",
      "Processing 2020 5.txt...\n",
      "Processing 2020 6.txt...\n",
      "Processing 2021 1.txt...\n",
      "Processing 2021 2.txt...\n",
      "Processing 2021 3.txt...\n",
      "Processing 2021 4.txt...\n",
      "Processing 2021 5.txt...\n",
      "Processing 2021 6.txt...\n",
      "Processing 2022 1.txt...\n",
      "Processing 2022 2.txt...\n",
      "Processing 2022 3.txt...\n",
      "Processing 2022 4.txt...\n",
      "Processing 2022 5.txt...\n",
      "Processing 2023 1.txt...\n",
      "Processing 2023 2.txt...\n",
      "Processing 2023 3.txt...\n",
      "Processing 2023 4.txt...\n",
      "Processing 2023 5.txt...\n",
      "Processing 2023 6.txt...\n",
      "Processing 2024 1.txt...\n",
      "Processing 2024 2.txt...\n",
      "Processing 2024 3.txt...\n",
      "Processing 2024 4.txt...\n",
      "Processing 2024 5.txt...\n",
      "Processing 2024 6.txt...\n",
      "Processing 2024 7.txt...\n",
      "Processing 2025 1.txt...\n",
      "Processing 2025 2.txt...\n",
      "Processing 2025 3.txt...\n",
      "Processing 2025 4.txt...\n",
      "Processing 2025 5.txt...\n",
      "\n",
      "Successfully processed 173999 articles!\n",
      "Data saved to: newspaper_data.csv\n",
      "Columns: ['source_file', 'title', 'author', 'publication_info', 'abstract', 'urls', 'links', 'publication_title', 'publication_date', 'publication_year', 'section', 'pages', 'publisher', 'place_of_publication', 'issn', 'document_type', 'proquest_id', 'database', 'full_text']\n",
      "\n",
      "First few rows preview:\n",
      "                                               title  \\\n",
      "0       'U' to host hockey regional: [METRO Edition]   \n",
      "1   Blankson makes his choice: UNLV: [Final Edition]   \n",
      "2  NCAA panel penalizes Cal football: [North Spor...   \n",
      "3  NBA DRAFT / After Dust Settles, Knicks Add Ill...   \n",
      "4               High schoolers sue NCAA over mandate   \n",
      "\n",
      "                                        author  \\\n",
      "0  Blount, Rachel; Staff Writer11 Staff Writer   \n",
      "1                                  Carp, Steve   \n",
      "2                                                \n",
      "3                   Jason Butler. STAFF WRITER   \n",
      "4                                                \n",
      "\n",
      "                               publication_title publication_date  \n",
      "0               Star Tribune; Minneapolis, Minn.     Jun 29, 2002  \n",
      "1    Las Vegas Review - Journal; Las Vegas, Nev.     Jun 28, 2002  \n",
      "2                 Chicago Tribune; Chicago, Ill.     Jun 27, 2002  \n",
      "3  Newsday, Combined editions; Long Island, N.Y.     Jun 27, 2002  \n",
      "4       Philadelphia Inquirer; Philadelphia, Pa.     Jun 27, 2002  \n",
      "\n",
      "Dataset statistics:\n",
      "- Total articles: 173999\n",
      "- Articles with abstracts: 173999\n",
      "- Unique publications: 257\n",
      "- Date range:  to Sep 9, 2024\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_newspaper_file(file_path):\n",
    "    \"\"\"Parse a single newspaper text file and extract structured data\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Split content by the separator lines\n",
    "    articles = content.split('_' * 60)  # Split by lines of underscores\n",
    "    \n",
    "    parsed_articles = []\n",
    "    \n",
    "    for article_text in articles:\n",
    "        article_text = article_text.strip()\n",
    "        if not article_text:  # Skip empty sections\n",
    "            continue\n",
    "            \n",
    "        article_data = {}\n",
    "        \n",
    "        # Extract title (first line that's not empty)\n",
    "        lines = article_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('Author:'):\n",
    "                article_data['title'] = line\n",
    "                break\n",
    "        \n",
    "        # Extract Author\n",
    "        author_match = re.search(r'Author:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['author'] = author_match.group(1).strip() if author_match else ''\n",
    "        \n",
    "        # Extract Publication info\n",
    "        pub_info_match = re.search(r'Publication info:\\s*(.+?)(?:\\n|http)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['publication_info'] = pub_info_match.group(1).strip() if pub_info_match else ''\n",
    "        \n",
    "        # Extract URLs (multiple URLs possible)\n",
    "        urls = re.findall(r'http[s]?://[^\\s\\n]+', article_text)\n",
    "        article_data['urls'] = ' | '.join(urls) if urls else ''\n",
    "        \n",
    "        # Extract Abstract\n",
    "        abstract_match = re.search(r'Abstract:\\s*(.+?)(?:\\nLinks:|\\nFull text:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['abstract'] = abstract_match.group(1).strip() if abstract_match else ''\n",
    "        \n",
    "        # Extract Links section\n",
    "        links_match = re.search(r'Links:\\s*(.+?)(?:\\nFull text:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['links'] = links_match.group(1).strip() if links_match else ''\n",
    "        \n",
    "        # Extract Full text\n",
    "        full_text_match = re.search(r'Full text:\\s*(.+?)(?:\\nCompany / organization:|People:|Title:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['full_text'] = full_text_match.group(1).strip() if full_text_match else ''\n",
    "        \n",
    "        # Extract company/ organization\n",
    "        company_match = re.search(r'Company / organization: Name:\\s*(.+?)(?:\\nTitle:|Publication title:|Pages:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['company_match'] = company_match.group(1).strip() if company_match else ''\n",
    "        \n",
    "        # Extract Title\n",
    "        title_match = re.search(r'Title:\\s*(.+?)(?:\\nPublication title:|Pages:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['title_match'] = title_match.group(1).strip() if title_match else ''\n",
    "        \n",
    "        # Extract Publication title\n",
    "        pub_title_match = re.search(r'Publication title:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_title'] = pub_title_match.group(1).strip() if pub_title_match else ''\n",
    "        \n",
    "        # Extract Pages\n",
    "        pages_match = re.search(r'Pages:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['pages'] = pages_match.group(1).strip() if pages_match else ''\n",
    "        \n",
    "        # Extract Publication year\n",
    "        pub_year_match = re.search(r'Publication year:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_year'] = pub_year_match.group(1).strip() if pub_year_match else ''\n",
    "        \n",
    "        # Extract Publication date\n",
    "        pub_date_match = re.search(r'Publication date:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_date'] = pub_date_match.group(1).strip() if pub_date_match else ''\n",
    "        \n",
    "        # Extract Section\n",
    "        section_match = re.search(r'Section:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['section'] = section_match.group(1).strip() if section_match else ''\n",
    "        \n",
    "        # Extract Publisher\n",
    "        publisher_match = re.search(r'Publisher:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publisher'] = publisher_match.group(1).strip() if publisher_match else ''\n",
    "        \n",
    "        # Extract Place of publication\n",
    "        place_match = re.search(r'Place of publication:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['place_of_publication'] = place_match.group(1).strip() if place_match else ''\n",
    "        \n",
    "        # Extract Publication subject\n",
    "        publication_subject_match = re.search(r'Publication subject:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_subject_match'] = publication_subject_match.group(1).strip() if publication_subject_match else ''\n",
    "        \n",
    "        # Extract ISSN\n",
    "        issn_match = re.search(r'ISSN:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['issn'] = issn_match.group(1).strip() if issn_match else ''\n",
    "        \n",
    "        # Extract Document type\n",
    "        doc_type_match = re.search(r'Document type:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['document_type'] = doc_type_match.group(1).strip() if doc_type_match else ''\n",
    "        \n",
    "        # Extract ProQuest document ID\n",
    "        doc_id_match = re.search(r'ProQuest document ID:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['proquest_id'] = doc_id_match.group(1).strip() if doc_id_match else ''\n",
    "        \n",
    "        # Extract Database\n",
    "        database_match = re.search(r'Database:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['database'] = database_match.group(1).strip() if database_match else ''\n",
    "        \n",
    "        # Add source file name\n",
    "        article_data['source_file'] = os.path.basename(file_path)\n",
    "        \n",
    "        # Only add if we have some meaningful data\n",
    "        if article_data.get('title') or article_data.get('author') or article_data.get('abstract'):\n",
    "            parsed_articles.append(article_data)\n",
    "    \n",
    "    return parsed_articles\n",
    "\n",
    "def process_all_files(folder_path=\"../data/NCAA_Newspaper_articles\", output_file=\"newspaper_data.csv\"):\n",
    "    \"\"\"Process all text files in the folder and create CSV\"\"\"\n",
    "    \n",
    "    # Get all text files\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(f\"No .txt files found in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(txt_files)} text files to process...\")\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        articles = parse_newspaper_file(file_path)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "    \n",
    "    if not all_articles:\n",
    "        print(\"No articles were successfully parsed.\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = [\n",
    "        'source_file', 'title', 'author', 'publication_info', 'abstract', \n",
    "        'urls', 'links', 'publication_title', 'publication_date', 'publication_year',\n",
    "        'section', 'pages', 'publisher', 'place_of_publication', 'issn',\n",
    "        'document_type', 'proquest_id', 'database', 'full_text'\n",
    "    ]\n",
    "    \n",
    "    # Reorder columns (only include columns that exist)\n",
    "    existing_columns = [col for col in column_order if col in df.columns]\n",
    "    df = df[existing_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(all_articles)} articles!\")\n",
    "    print(f\"Data saved to: {output_file}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst few rows preview:\")\n",
    "    print(df[['title', 'author', 'publication_title', 'publication_date']].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all files and create CSV\n",
    "    df = process_all_files()\n",
    "    \n",
    "    # Optional: Display some statistics\n",
    "    if df is not None:\n",
    "        print(f\"\\nDataset statistics:\")\n",
    "        print(f\"- Total articles: {len(df)}\")\n",
    "        print(f\"- Articles with abstracts: {df['abstract'].notna().sum()}\")\n",
    "        print(f\"- Unique publications: {df['publication_title'].nunique()}\")\n",
    "        print(f\"- Date range: {df['publication_date'].min()} to {df['publication_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6c7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191 text files to process...\n",
      "Processing 2002 1.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 10.txt...\n",
      "  Found 1000 articles for year 2002\n",
      "Processing 2002 11.txt...\n",
      "  Found 649 articles for year 2002\n",
      "Processing 2002 2.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 3.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 4.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 5.txt...\n",
      "  Found 1002 articles for year 2002\n",
      "Processing 2002 6.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 7.txt...\n",
      "  Found 501 articles for year 2002\n",
      "Processing 2002 8.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2002 9.txt...\n",
      "  Found 1001 articles for year 2002\n",
      "Processing 2003 1.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 10.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 11.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 12.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 13.txt...\n",
      "  Found 23 articles for year 2003\n",
      "Processing 2003 2.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 3.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 4.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 5.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 6.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 7.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2003 8.txt...\n",
      "  Found 306 articles for year 2003\n",
      "Processing 2003 9.txt...\n",
      "  Found 1001 articles for year 2003\n",
      "Processing 2004 1.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 10.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 11.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 12.txt...\n",
      "  Found 384 articles for year 2004\n",
      "Processing 2004 2.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 3.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 4.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 5.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 6.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 7 .txt...\n",
      "  Found 456 articles for year 2004\n",
      "Processing 2004 8.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2004 9.txt...\n",
      "  Found 1001 articles for year 2004\n",
      "Processing 2005 1.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 10.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 11.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 12.txt...\n",
      "  Found 460 articles for year 2005\n",
      "Processing 2005 2.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 3.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 4.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 5.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 6.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 7.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2005 8.txt...\n",
      "  Found 102 articles for year 2005\n",
      "Processing 2005 9.txt...\n",
      "  Found 1001 articles for year 2005\n",
      "Processing 2006 1.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 10.txt...\n",
      "  Found 796 articles for year 2006\n",
      "Processing 2006 2.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 3.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 4.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 5.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 6.txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 7 .txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 8 .txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2006 9 .txt...\n",
      "  Found 1001 articles for year 2006\n",
      "Processing 2007 1.txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 10.txt...\n",
      "  Found 671 articles for year 2007\n",
      "Processing 2007 2.txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 3.txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 4.txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 5.txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 6 .txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 8 .txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2007 9 .txt...\n",
      "  Found 1001 articles for year 2007\n",
      "Processing 2008 1.txt...\n",
      "  Found 999 articles for year 2008\n",
      "Processing 2008 2.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 3.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 4.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 5.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 6.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 7.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 8.txt...\n",
      "  Found 1001 articles for year 2008\n",
      "Processing 2008 9.txt...\n",
      "  Found 484 articles for year 2008\n",
      "Processing 2009 1.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 2.txt...\n",
      "  Found 999 articles for year 2009\n",
      "Processing 2009 3.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 4.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 5.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 6.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 7.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 8.txt...\n",
      "  Found 1001 articles for year 2009\n",
      "Processing 2009 9.txt...\n",
      "  Found 17 articles for year 2009\n",
      "Processing 2010 1.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 2.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 3.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 4.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 5.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 6.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 7.txt...\n",
      "  Found 1001 articles for year 2010\n",
      "Processing 2010 8.txt...\n",
      "  Found 971 articles for year 2010\n",
      "Processing 2011 1.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 2.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 3.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 4.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 5.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 6.txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 7 .txt...\n",
      "  Found 1001 articles for year 2011\n",
      "Processing 2011 8.txt...\n",
      "  Found 447 articles for year 2011\n",
      "Processing 2012 1.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 2.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 3.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 4.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 5.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 6.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 7.txt...\n",
      "  Found 1001 articles for year 2012\n",
      "Processing 2012 8.txt...\n",
      "  Found 710 articles for year 2012\n",
      "Processing 2013 1.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 2.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 3.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 4.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 5.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 6.txt...\n",
      "  Found 1001 articles for year 2013\n",
      "Processing 2013 7.txt...\n",
      "  Found 538 articles for year 2013\n",
      "Processing 2014 1.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 2.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 3.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 4.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 5.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 6.txt...\n",
      "  Found 1001 articles for year 2014\n",
      "Processing 2014 7.txt...\n",
      "  Found 26 articles for year 2014\n",
      "Processing 2015 1.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 2.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 3.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 4.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 5.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 6.txt...\n",
      "  Found 1001 articles for year 2015\n",
      "Processing 2015 7.txt...\n",
      "  Found 142 articles for year 2015\n",
      "Processing 2016 1.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 2.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 3.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 4.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 5.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 6.txt...\n",
      "  Found 1001 articles for year 2016\n",
      "Processing 2016 7.txt...\n",
      "  Found 24 articles for year 2016\n",
      "Processing 2017 1.txt...\n",
      "  Found 1001 articles for year 2017\n",
      "Processing 2017 2.txt...\n",
      "  Found 1001 articles for year 2017\n",
      "Processing 2017 3.txt...\n",
      "  Found 1001 articles for year 2017\n",
      "Processing 2017 4.txt...\n",
      "  Found 1001 articles for year 2017\n",
      "Processing 2017 5.txt...\n",
      "  Found 1001 articles for year 2017\n",
      "Processing 2017 6.txt...\n",
      "  Found 632 articles for year 2017\n",
      "Processing 2018 1.txt...\n",
      "  Found 1001 articles for year 2018\n",
      "Processing 2018 2.txt...\n",
      "  Found 1001 articles for year 2018\n",
      "Processing 2018 3.txt...\n",
      "  Found 1001 articles for year 2018\n",
      "Processing 2018 4.txt...\n",
      "  Found 1001 articles for year 2018\n",
      "Processing 2018 5.txt...\n",
      "  Found 1001 articles for year 2018\n",
      "Processing 2018 6.txt...\n",
      "  Found 678 articles for year 2018\n",
      "Processing 2019 1.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 2.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 3.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 4.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 5.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 6.txt...\n",
      "  Found 1001 articles for year 2019\n",
      "Processing 2019 7.txt...\n",
      "  Found 246 articles for year 2019\n",
      "Processing 2020 1.txt...\n",
      "  Found 1001 articles for year 2020\n",
      "Processing 2020 2.txt...\n",
      "  Found 1001 articles for year 2020\n",
      "Processing 2020 3.txt...\n",
      "  Found 1001 articles for year 2020\n",
      "Processing 2020 4.txt...\n",
      "  Found 1001 articles for year 2020\n",
      "Processing 2020 5.txt...\n",
      "  Found 1001 articles for year 2020\n",
      "Processing 2020 6.txt...\n",
      "  Found 37 articles for year 2020\n",
      "Processing 2021 1.txt...\n",
      "  Found 1001 articles for year 2021\n",
      "Processing 2021 2.txt...\n",
      "  Found 1001 articles for year 2021\n",
      "Processing 2021 3.txt...\n",
      "  Found 1001 articles for year 2021\n",
      "Processing 2021 4.txt...\n",
      "  Found 1001 articles for year 2021\n",
      "Processing 2021 5.txt...\n",
      "  Found 901 articles for year 2021\n",
      "Processing 2021 6.txt...\n",
      "  Found 440 articles for year 2021\n",
      "Processing 2022 1.txt...\n",
      "  Found 1001 articles for year 2022\n",
      "Processing 2022 2.txt...\n",
      "  Found 1001 articles for year 2022\n",
      "Processing 2022 3.txt...\n",
      "  Found 1001 articles for year 2022\n",
      "Processing 2022 4.txt...\n",
      "  Found 1001 articles for year 2022\n",
      "Processing 2022 5.txt...\n",
      "  Found 636 articles for year 2022\n",
      "Processing 2023 1.txt...\n",
      "  Found 1001 articles for year 2023\n",
      "Processing 2023 2.txt...\n",
      "  Found 1001 articles for year 2023\n",
      "Processing 2023 3.txt...\n",
      "  Found 1001 articles for year 2023\n",
      "Processing 2023 4.txt...\n",
      "  Found 1001 articles for year 2023\n",
      "Processing 2023 5.txt...\n",
      "  Found 1001 articles for year 2023\n",
      "Processing 2023 6.txt...\n",
      "  Found 23 articles for year 2023\n",
      "Processing 2024 1.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 2.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 3.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 4.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 5.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 6.txt...\n",
      "  Found 1001 articles for year 2024\n",
      "Processing 2024 7.txt...\n",
      "  Found 386 articles for year 2024\n",
      "Processing 2025 1.txt...\n",
      "  Found 1001 articles for year 2025\n",
      "Processing 2025 2.txt...\n",
      "  Found 1001 articles for year 2025\n",
      "Processing 2025 3.txt...\n",
      "  Found 1001 articles for year 2025\n",
      "Processing 2025 4.txt...\n",
      "  Found 1001 articles for year 2025\n",
      "Processing 2025 5.txt...\n",
      "  Found 155 articles for year 2025\n",
      "\n",
      "Processing 10159 articles for year 2002...\n",
      "Saved output_by_year\\newspaper_data_2002.csv: 10159 articles, 71.00 MB\n",
      "Sample from 2002:\n",
      "                                              title  \\\n",
      "0      'U' to host hockey regional: [METRO Edition]   \n",
      "1  Blankson makes his choice: UNLV: [Final Edition]   \n",
      "\n",
      "                                        author  \\\n",
      "0  Blount, Rachel; Staff Writer11 Staff Writer   \n",
      "1                                  Carp, Steve   \n",
      "\n",
      "                             publication_title publication_date  \n",
      "0             Star Tribune; Minneapolis, Minn.     Jun 29, 2002  \n",
      "1  Las Vegas Review - Journal; Las Vegas, Nev.     Jun 28, 2002  \n",
      "\n",
      "Processing 11340 articles for year 2003...\n",
      "Saved output_by_year\\newspaper_data_2003.csv: 11340 articles, 76.65 MB\n",
      "Sample from 2003:\n",
      "                                               title  \\\n",
      "0  BEHIND THE TROPHIES IN THE ATHLETIC DEPARTMENT...   \n",
      "1   Twins won't run away with title: [METRO Edition]   \n",
      "\n",
      "                                      author  \\\n",
      "0  Dean, Spiros; Staff Writer11 Staff Writer   \n",
      "1  Hartman, Sid; Staff Writer11 Staff Writer   \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0  Star Tribune; Minneapolis, Minn.     Jun 30, 2003  \n",
      "1  Star Tribune; Minneapolis, Minn.     Jun 29, 2003  \n",
      "\n",
      "Processing 10850 articles for year 2004...\n",
      "Saved output_by_year\\newspaper_data_2004.csv: 10850 articles, 75.66 MB\n",
      "Sample from 2004:\n",
      "                                               title  \\\n",
      "0        NCAA makes it tough on 'U': [METRO Edition]   \n",
      "1  A Brilliant Game -- and That's Enough: [FINAL ...   \n",
      "\n",
      "                                      author  \\\n",
      "0  Dean, Spiros; Staff Writer11 Staff Writer   \n",
      "1                                 Wise, Mike   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0       Star Tribune; Minneapolis, Minn.      Jun 1, 2004  \n",
      "1  The Washington Post; Washington, D.C.      Jun 1, 2004  \n",
      "\n",
      "Processing 10572 articles for year 2005...\n",
      "Saved output_by_year\\newspaper_data_2005.csv: 10572 articles, 70.42 MB\n",
      "Sample from 2005:\n",
      "                                               title  \\\n",
      "0  Q AND A ; CARLYLE CARTER, OUTGOING MIAC EXECUT...   \n",
      "1  It's All Relative; Everyone has a big story at...   \n",
      "\n",
      "                                              author  \\\n",
      "0  Brackin, Dennis; Rand, Michael; Staff Writers1...   \n",
      "1                                        Dwyre, Bill   \n",
      "\n",
      "                        publication_title publication_date  \n",
      "0        Star Tribune; Minneapolis, Minn.     Jun 30, 2005  \n",
      "1  Los Angeles Times; Los Angeles, Calif.     Jun 30, 2005  \n",
      "\n",
      "Processing 9805 articles for year 2006...\n",
      "Saved output_by_year\\newspaper_data_2006.csv: 9805 articles, 67.41 MB\n",
      "Sample from 2006:\n",
      "                                               title  \\\n",
      "0  Basketball Recruiting on the Nonprofit Margins...   \n",
      "1  A coach who knows hairy situations ; Jim White...   \n",
      "\n",
      "                                         author  \\\n",
      "0  Eric Prisbell - Washington Post Staff Writer   \n",
      "1                                 Freedman, Lew   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0  The Washington Post; Washington, D.C.     Dec 31, 2006  \n",
      "1         Chicago Tribune; Chicago, Ill.     Dec 29, 2006  \n",
      "\n",
      "Processing 8679 articles for year 2007...\n",
      "Saved output_by_year\\newspaper_data_2007.csv: 8679 articles, 57.81 MB\n",
      "Sample from 2007:\n",
      "                                               title         author  \\\n",
      "0  Obituaries / Tab Thacker, 1962 - 2007; NCAA wr...      Anonymous   \n",
      "1  Minutemen hold off BU: [Terriers' rally comes ...  Dobrow, Marty   \n",
      "\n",
      "                        publication_title publication_date  \n",
      "0  Los Angeles Times; Los Angeles, Calif.     Dec 30, 2007  \n",
      "1             Boston Globe; Boston, Mass.     Dec 30, 2007  \n",
      "\n",
      "Processing 8490 articles for year 2008...\n",
      "Saved output_by_year\\newspaper_data_2008.csv: 8490 articles, 51.53 MB\n",
      "Sample from 2008:\n",
      "                                               title         author  \\\n",
      "0                       Dayton Rallies To Beat Mason      Anonymous   \n",
      "1  Defense to drive this opening trip: No. 9 Purd...  Bannon, Terry   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0  The Washington Post; Washington, D.C.     Dec 31, 2008  \n",
      "1         Chicago Tribune; Chicago, Ill.     Dec 30, 2008  \n",
      "\n",
      "Processing 8023 articles for year 2009...\n",
      "Saved output_by_year\\newspaper_data_2009.csv: 8023 articles, 48.74 MB\n",
      "Sample from 2009:\n",
      "                                               title           author  \\\n",
      "0  USC's three reasons for crying foul; Some in t...   Wharton, David   \n",
      "1  2 seniors on a mission: ND's Harangody, Jackso...  Hamilton, Brian   \n",
      "\n",
      "                        publication_title publication_date  \n",
      "0  Los Angeles Times; Los Angeles, Calif.     Dec 31, 2009  \n",
      "1          Chicago Tribune; Chicago, Ill.     Dec 30, 2009  \n",
      "\n",
      "Processing 7978 articles for year 2010...\n",
      "Saved output_by_year\\newspaper_data_2010.csv: 7978 articles, 48.18 MB\n",
      "Sample from 2010:\n",
      "                                               title          author  \\\n",
      "0  Big Ten success key to NU quest: Purdue 1st hu...       Anonymous   \n",
      "1                      NCAA is sending wrong message  Jenkins, Sally   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0         Chicago Tribune; Chicago, Ill.     Dec 31, 2010  \n",
      "1  The Washington Post; Washington, D.C.     Dec 31, 2010  \n",
      "\n",
      "Processing 7454 articles for year 2011...\n",
      "Saved output_by_year\\newspaper_data_2011.csv: 7454 articles, 45.27 MB\n",
      "Sample from 2011:\n",
      "                                               title      author  \\\n",
      "0                      Inspiration Moves Off the Mat  Gay, Jason   \n",
      "1  2011: DOGS HAVE THEIR DAYS ON NCAA'S BIGGEST S...               \n",
      "\n",
      "                                   publication_title publication_date  \n",
      "0  Wall Street Journal, Eastern edition; New York...     Dec 29, 2011  \n",
      "1                             USA TODAY; McLean, Va.     Dec 28, 2011  \n",
      "\n",
      "Processing 7717 articles for year 2012...\n",
      "Saved output_by_year\\newspaper_data_2012.csv: 7717 articles, 47.32 MB\n",
      "Sample from 2012:\n",
      "                                               title           author  \\\n",
      "0  U is sharp in tuneup for showdown vs. BC: The ...   Russo, Michael   \n",
      "1  Penn State and NCAA at odds over fate of fines...  Roebuck, Jeremy   \n",
      "\n",
      "                          publication_title publication_date  \n",
      "0          Star Tribune; Minneapolis, Minn.     Dec 30, 2012  \n",
      "1  Philadelphia Inquirer; Philadelphia, Pa.     Dec 30, 2012  \n",
      "\n",
      "Processing 6544 articles for year 2013...\n",
      "Saved output_by_year\\newspaper_data_2013.csv: 6544 articles, 40.71 MB\n",
      "Sample from 2013:\n",
      "                                               title         author  \\\n",
      "0  2DAYS 2 CENTS: OUR TOP 10 IN A YEAROF MIDDLING...  Rand, Michael   \n",
      "1  mens big ten preview: BADGERS AIM HIGH; Wiscon...  RAYNO, AMELIA   \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0  Star Tribune; Minneapolis, Minn.     Dec 31, 2013  \n",
      "1  Star Tribune; Minneapolis, Minn.     Dec 31, 2013  \n",
      "\n",
      "Processing 6032 articles for year 2014...\n",
      "Saved output_by_year\\newspaper_data_2014.csv: 6032 articles, 38.31 MB\n",
      "Sample from 2014:\n",
      "                                               title         author  \\\n",
      "0  Are Gophers NCAA-worthy? The test begins: U no...  RAYNO, AMELIA   \n",
      "1  2DAY: PETERSON IS NO. 1 STORY, AND NOT IN A GO...  Rand, Michael   \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0  Star Tribune; Minneapolis, Minn.     Dec 31, 2014  \n",
      "1  Star Tribune; Minneapolis, Minn.     Dec 31, 2014  \n",
      "\n",
      "Processing 6148 articles for year 2015...\n",
      "Saved output_by_year\\newspaper_data_2015.csv: 6148 articles, 39.71 MB\n",
      "Sample from 2015:\n",
      "                                               title        author  \\\n",
      "0  College athletics spend big in branding: Costl...  Axon, Rachel   \n",
      "1                         BIG TEN BASKETBALL PREVIEW                 \n",
      "\n",
      "                publication_title publication_date  \n",
      "0          USA TODAY; McLean, Va.     Dec 30, 2015  \n",
      "1  Chicago Tribune; Chicago, Ill.     Dec 29, 2015  \n",
      "\n",
      "Processing 6030 articles for year 2016...\n",
      "Saved output_by_year\\newspaper_data_2016.csv: 6030 articles, 40.43 MB\n",
      "Sample from 2016:\n",
      "                                               title  \\\n",
      "0  INSIDER: NEVER A DULL MOMENT IN ACC; CONTROVER...   \n",
      "1  NCAA Football: In College Football, It's Alaba...   \n",
      "\n",
      "                       author  \\\n",
      "0              Fuller, Marcus   \n",
      "1  Beaton, Andrew; Cohen, Ben   \n",
      "\n",
      "                                   publication_title publication_date  \n",
      "0                   Star Tribune; Minneapolis, Minn.     Dec 31, 2016  \n",
      "1  Wall Street Journal, Eastern edition; New York...     Dec 31, 2016  \n",
      "\n",
      "Processing 5637 articles for year 2017...\n",
      "Saved output_by_year\\newspaper_data_2017.csv: 5637 articles, 38.76 MB\n",
      "Sample from 2017:\n",
      "                                               title           author  \\\n",
      "0  Six years after sandusky, Penn State is at war...     Hobson, Will   \n",
      "1  UCLA football collapses in Cactus Bowl against...  Thuc Nhi Nguyen   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0  The Washington Post; Washington, D.C.     Dec 29, 2017  \n",
      "1        Daily News; Los Angeles, Calif.     Dec 27, 2017  \n",
      "\n",
      "Processing 5683 articles for year 2018...\n",
      "Saved output_by_year\\newspaper_data_2018.csv: 5683 articles, 39.80 MB\n",
      "Sample from 2018:\n",
      "                                               title          author  \\\n",
      "0  College bowl schedule: Breaking down Monday's ...    Fiutak, Pete   \n",
      "1  Can Pac-12 produce 1st NCAA women's champ sinc...  Metcalfe, Jeff   \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0     USA Today (Online); Arlington     Dec 31, 2018  \n",
      "1  Arizona Republic; Phoenix, Ariz.     Dec 30, 2018  \n",
      "\n",
      "Processing 6252 articles for year 2019...\n",
      "Saved output_by_year\\newspaper_data_2019.csv: 6252 articles, 44.74 MB\n",
      "Sample from 2019:\n",
      "                                               title  \\\n",
      "0  Bowl swag not for sale: A look at NCAA limitat...   \n",
      "1  How the NCAA's new 3-point line is affecting c...   \n",
      "\n",
      "                                      author              publication_title  \\\n",
      "0                             Gardner, Hayes  USA Today (Online); Arlington   \n",
      "1  Hayes Gardner; Louisville Courier Journal  USA Today (Online); Arlington   \n",
      "\n",
      "  publication_date  \n",
      "0     Dec 27, 2019  \n",
      "1     Dec 27, 2019  \n",
      "\n",
      "Processing 5042 articles for year 2020...\n",
      "Saved output_by_year\\newspaper_data_2020.csv: 5042 articles, 38.15 MB\n",
      "Sample from 2020:\n",
      "                                               title           author  \\\n",
      "0  Pandemic cut off many AZ athletes from their s...   Metcalfe, Jeff   \n",
      "1  Why Four Teams Dominate the Playoff --- Deep p...  Bachman, Rachel   \n",
      "\n",
      "                                   publication_title publication_date  \n",
      "0                   Arizona Republic; Phoenix, Ariz.    Dec 31, 202 0  \n",
      "1  Wall Street Journal, Eastern edition; New York...     Dec 31, 2020  \n",
      "\n",
      "Processing 5345 articles for year 2021...\n",
      "Saved output_by_year\\newspaper_data_2021.csv: 5345 articles, 40.21 MB\n",
      "Sample from 2021:\n",
      "                                               title author  \\\n",
      "0                                      NCAA playoffs          \n",
      "1  SUPER PREPS 365: ON TO THE NEXT LEVEL ; THE EA...          \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0            USA TODAY; McLean, Va.     Dec 17, 2021  \n",
      "1  Star Tribune; Minneapolis, Minn.     Dec 15, 2021  \n",
      "\n",
      "Processing 4640 articles for year 2022...\n",
      "Saved output_by_year\\newspaper_data_2022.csv: 4640 articles, 35.51 MB\n",
      "Sample from 2022:\n",
      "                                               title            author  \\\n",
      "0                Ex-Farmington star to transfer to U  Youngblood, Kent   \n",
      "1  It's time for the NCAA to return the Heisman i...    Jenkins, Sally   \n",
      "\n",
      "                       publication_title publication_date  \n",
      "0       Star Tribune; Minneapolis, Minn.     Dec 30, 2022  \n",
      "1  The Washington Post; Washington, D.C.     Dec 29, 2022  \n",
      "\n",
      "Processing 5028 articles for year 2023...\n",
      "Saved output_by_year\\newspaper_data_2023.csv: 5028 articles, 37.68 MB\n",
      "Sample from 2023:\n",
      "                                               title       author  \\\n",
      "0  Yes, Michigan's Jim Harbaugh can be odd and fr...  Wolken, Dan   \n",
      "1  In a sense, college football has transformed i...                \n",
      "\n",
      "               publication_title publication_date  \n",
      "0  USA Today (Online); Arlington     Dec 31, 2023  \n",
      "1     Tampa Bay Times; Tampa Bay     Dec 31, 2023  \n",
      "\n",
      "Processing 6392 articles for year 2024...\n",
      "Saved output_by_year\\newspaper_data_2024.csv: 6392 articles, 48.26 MB\n",
      "Sample from 2024:\n",
      "                                               title         author  \\\n",
      "0  Kenny Dillingham makes Arizona State evolution...    Wolken, Dan   \n",
      "1  What is the NCAA rushing record? Can Ashton Je...  Kassim, Ehsan   \n",
      "\n",
      "               publication_title publication_date  \n",
      "0  USA Today (Online); Arlington     Dec 31, 2024  \n",
      "1  USA Today (Online); Arlington     Dec 31, 2024  \n",
      "\n",
      "Processing 4159 articles for year 2025...\n",
      "Saved output_by_year\\newspaper_data_2025.csv: 4159 articles, 28.83 MB\n",
      "Sample from 2025:\n",
      "                                               title        author  \\\n",
      "0  Arizona State men's golf aims for NCAA tourney...                 \n",
      "1         Arizona, ASU programs reach NCAA regionals  Ortiz, Jenna   \n",
      "\n",
      "                  publication_title publication_date  \n",
      "0  Arizona Republic; Phoenix, Ariz.     May 27, 2025  \n",
      "1  Arizona Republic; Phoenix, Ariz.     May 27, 2025  \n",
      "\n",
      "==================================================\n",
      "SUMMARY STATISTICS BY YEAR\n",
      "==================================================\n",
      "Year 2002:\n",
      "  - Total articles: 10159\n",
      "  - Articles with abstracts: 10159\n",
      "  - Unique publications: 33\n",
      "  - Date range:  to Sep 9, 2002\n",
      "\n",
      "Year 2003:\n",
      "  - Total articles: 11340\n",
      "  - Articles with abstracts: 11340\n",
      "  - Unique publications: 33\n",
      "  - Date range:  to Sep 9, 2003\n",
      "\n",
      "Year 2004:\n",
      "  - Total articles: 10850\n",
      "  - Articles with abstracts: 10850\n",
      "  - Unique publications: 38\n",
      "  - Date range:  to Sep 9, 2004\n",
      "\n",
      "Year 2005:\n",
      "  - Total articles: 10572\n",
      "  - Articles with abstracts: 10572\n",
      "  - Unique publications: 36\n",
      "  - Date range:  to Sep 9, 2005\n",
      "\n",
      "Year 2006:\n",
      "  - Total articles: 9805\n",
      "  - Articles with abstracts: 9805\n",
      "  - Unique publications: 42\n",
      "  - Date range:  to Sep 9, 2006\n",
      "\n",
      "Year 2007:\n",
      "  - Total articles: 8679\n",
      "  - Articles with abstracts: 8679\n",
      "  - Unique publications: 27\n",
      "  - Date range:  to Sep 9, 2007\n",
      "\n",
      "Year 2008:\n",
      "  - Total articles: 8490\n",
      "  - Articles with abstracts: 8490\n",
      "  - Unique publications: 19\n",
      "  - Date range:  to Sep 9, 2008\n",
      "\n",
      "Year 2009:\n",
      "  - Total articles: 8023\n",
      "  - Articles with abstracts: 8023\n",
      "  - Unique publications: 23\n",
      "  - Date range:  to Sep 9, 2009\n",
      "\n",
      "Year 2010:\n",
      "  - Total articles: 7978\n",
      "  - Articles with abstracts: 7978\n",
      "  - Unique publications: 19\n",
      "  - Date range:  to Sep 9, 2010\n",
      "\n",
      "Year 2011:\n",
      "  - Total articles: 7454\n",
      "  - Articles with abstracts: 7454\n",
      "  - Unique publications: 23\n",
      "  - Date range:  to Sep 9, 2011\n",
      "\n",
      "Year 2012:\n",
      "  - Total articles: 7717\n",
      "  - Articles with abstracts: 7717\n",
      "  - Unique publications: 25\n",
      "  - Date range:  to Sep 9, 2012\n",
      "\n",
      "Year 2013:\n",
      "  - Total articles: 6544\n",
      "  - Articles with abstracts: 6544\n",
      "  - Unique publications: 26\n",
      "  - Date range:  to Sep 9, 2013\n",
      "\n",
      "Year 2014:\n",
      "  - Total articles: 6032\n",
      "  - Articles with abstracts: 6032\n",
      "  - Unique publications: 29\n",
      "  - Date range:  to Sep 9, 2014\n",
      "\n",
      "Year 2015:\n",
      "  - Total articles: 6148\n",
      "  - Articles with abstracts: 6148\n",
      "  - Unique publications: 26\n",
      "  - Date range:  to Sep 9, 2015\n",
      "\n",
      "Year 2016:\n",
      "  - Total articles: 6030\n",
      "  - Articles with abstracts: 6030\n",
      "  - Unique publications: 23\n",
      "  - Date range:  to Sep 9, 2016\n",
      "\n",
      "Year 2017:\n",
      "  - Total articles: 5637\n",
      "  - Articles with abstracts: 5637\n",
      "  - Unique publications: 44\n",
      "  - Date range:  to Sep 9, 2017\n",
      "\n",
      "Year 2018:\n",
      "  - Total articles: 5683\n",
      "  - Articles with abstracts: 5683\n",
      "  - Unique publications: 60\n",
      "  - Date range:  to Sep 9, 2018\n",
      "\n",
      "Year 2019:\n",
      "  - Total articles: 6252\n",
      "  - Articles with abstracts: 6252\n",
      "  - Unique publications: 61\n",
      "  - Date range:  to Sep 9, 2019\n",
      "\n",
      "Year 2020:\n",
      "  - Total articles: 5042\n",
      "  - Articles with abstracts: 5042\n",
      "  - Unique publications: 38\n",
      "  - Date range:  to Sep 9, 2020\n",
      "\n",
      "Year 2021:\n",
      "  - Total articles: 5345\n",
      "  - Articles with abstracts: 5345\n",
      "  - Unique publications: 53\n",
      "  - Date range:  to Sep 9, 2021\n",
      "\n",
      "Year 2022:\n",
      "  - Total articles: 4640\n",
      "  - Articles with abstracts: 4640\n",
      "  - Unique publications: 39\n",
      "  - Date range:  to Sep 9, 2022\n",
      "\n",
      "Year 2023:\n",
      "  - Total articles: 5028\n",
      "  - Articles with abstracts: 5028\n",
      "  - Unique publications: 47\n",
      "  - Date range:  to Sep 9, 2023\n",
      "\n",
      "Year 2024:\n",
      "  - Total articles: 6392\n",
      "  - Articles with abstracts: 6392\n",
      "  - Unique publications: 37\n",
      "  - Date range:  to Sep 9, 2024\n",
      "\n",
      "Year 2025:\n",
      "  - Total articles: 4159\n",
      "  - Articles with abstracts: 4159\n",
      "  - Unique publications: 33\n",
      "  - Date range:  to May 9, 2025\n",
      "\n",
      "TOTAL ARTICLES ACROSS ALL YEARS: 173999\n",
      "CSV files created in: output_by_year/\n",
      "Created yearly summary file: output_by_year/yearly_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_newspaper_file(file_path):\n",
    "    \"\"\"Parse a single newspaper text file and extract structured data\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Split content by the separator lines\n",
    "    articles = content.split('_' * 60)  # Split by lines of underscores\n",
    "    \n",
    "    parsed_articles = []\n",
    "    \n",
    "    for article_text in articles:\n",
    "        article_text = article_text.strip()\n",
    "        if not article_text:  # Skip empty sections\n",
    "            continue\n",
    "             \n",
    "        article_data = {}\n",
    "        \n",
    "        # Extract title (first line that's not empty)\n",
    "        lines = article_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('Author:'):\n",
    "                article_data['title'] = line\n",
    "                break\n",
    "        \n",
    "        # Extract Author\n",
    "        author_match = re.search(r'Author:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['author'] = author_match.group(1).strip() if author_match else ''\n",
    "        \n",
    "        # Extract Publication info\n",
    "        pub_info_match = re.search(r'Publication info:\\s*(.+?)(?:\\n|http)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['publication_info'] = pub_info_match.group(1).strip() if pub_info_match else ''\n",
    "        \n",
    "        # Extract URLs (multiple URLs possible)\n",
    "        urls = re.findall(r'http[s]?://[^\\s\\n]+', article_text)\n",
    "        article_data['urls'] = ' | '.join(urls) if urls else ''\n",
    "        \n",
    "        # Extract Abstract\n",
    "        abstract_match = re.search(r'Abstract:\\s*(.+?)(?:\\nLinks:|\\nFull text:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['abstract'] = abstract_match.group(1).strip() if abstract_match else ''\n",
    "        \n",
    "        # Extract Links section\n",
    "        links_match = re.search(r'Links:\\s*(.+?)(?:\\nFull text:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['links'] = links_match.group(1).strip() if links_match else ''\n",
    "        \n",
    "        # Extract Full text\n",
    "        full_text_match = re.search(r'Full text:\\s*(.+?)(?:\\nCompany / organization:|People:|Title:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['full_text'] = full_text_match.group(1).strip() if full_text_match else ''\n",
    "        \n",
    "        # Extract company/ organization\n",
    "        company_match = re.search(r'Company / organization: Name:\\s*(.+?)(?:\\nTitle:|Publication title:|Pages:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['company_match'] = company_match.group(1).strip() if company_match else ''\n",
    "        \n",
    "        # Extract Title\n",
    "        title_match = re.search(r'Title:\\s*(.+?)(?:\\nPublication title:|Pages:)', article_text, re.MULTILINE | re.DOTALL)\n",
    "        article_data['title_match'] = title_match.group(1).strip() if title_match else ''\n",
    "        \n",
    "        # Extract Publication title\n",
    "        pub_title_match = re.search(r'Publication title:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_title'] = pub_title_match.group(1).strip() if pub_title_match else ''\n",
    "        \n",
    "        # Extract Pages\n",
    "        pages_match = re.search(r'Pages:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['pages'] = pages_match.group(1).strip() if pages_match else ''\n",
    "        \n",
    "        # Extract Publication year\n",
    "        pub_year_match = re.search(r'Publication year:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_year'] = pub_year_match.group(1).strip() if pub_year_match else ''\n",
    "        \n",
    "        # Extract Publication date\n",
    "        pub_date_match = re.search(r'Publication date:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_date'] = pub_date_match.group(1).strip() if pub_date_match else ''\n",
    "        \n",
    "        # Extract Section\n",
    "        section_match = re.search(r'Section:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['section'] = section_match.group(1).strip() if section_match else ''\n",
    "        \n",
    "        # Extract Publisher\n",
    "        publisher_match = re.search(r'Publisher:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publisher'] = publisher_match.group(1).strip() if publisher_match else ''\n",
    "        \n",
    "        # Extract Place of publication\n",
    "        place_match = re.search(r'Place of publication:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['place_of_publication'] = place_match.group(1).strip() if place_match else ''\n",
    "        \n",
    "        # Extract Publication subject\n",
    "        publication_subject_match = re.search(r'Publication subject:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['publication_subject_match'] = publication_subject_match.group(1).strip() if publication_subject_match else ''\n",
    "        \n",
    "        # Extract ISSN\n",
    "        issn_match = re.search(r'ISSN:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['issn'] = issn_match.group(1).strip() if issn_match else ''\n",
    "        \n",
    "        # Extract Document type\n",
    "        doc_type_match = re.search(r'Document type:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['document_type'] = doc_type_match.group(1).strip() if doc_type_match else ''\n",
    "        \n",
    "        # Extract ProQuest document ID\n",
    "        doc_id_match = re.search(r'ProQuest document ID:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['proquest_id'] = doc_id_match.group(1).strip() if doc_id_match else ''\n",
    "        \n",
    "        # Extract Database\n",
    "        database_match = re.search(r'Database:\\s*(.+?)(?:\\n|$)', article_text, re.MULTILINE)\n",
    "        article_data['database'] = database_match.group(1).strip() if database_match else ''\n",
    "        \n",
    "        # Add source file name\n",
    "        article_data['source_file'] = os.path.basename(file_path)\n",
    "        \n",
    "        # Only add if we have some meaningful data\n",
    "        if article_data.get('title') or article_data.get('author') or article_data.get('abstract'):\n",
    "            parsed_articles.append(article_data)\n",
    "    \n",
    "    return parsed_articles\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    \"\"\"Extract year from filename like '2002 1.txt' or '2024 2.txt'\"\"\"\n",
    "    # Use regex to find 4-digit year at the start of filename\n",
    "    year_match = re.match(r'(\\d{4})', filename)\n",
    "    if year_match:\n",
    "        return year_match.group(1)\n",
    "    return None\n",
    "\n",
    "def process_all_files_by_year(folder_path=\"../data/NCAA_Newspaper_articles\", output_folder=\"output_by_year\"):\n",
    "    \"\"\"Process all text files and create separate CSV files for each year\"\"\"\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Get all text files\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(f\"No .txt files found in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(txt_files)} text files to process...\")\n",
    "    \n",
    "    # Dictionary to store articles by year\n",
    "    articles_by_year = defaultdict(list)\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in txt_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        # Extract year from filename\n",
    "        year = extract_year_from_filename(filename)\n",
    "        if not year:\n",
    "            print(f\"  Warning: Could not extract year from {filename}\")\n",
    "            year = \"unknown\"\n",
    "        \n",
    "        # Parse the file\n",
    "        articles = parse_newspaper_file(file_path)\n",
    "        if articles:\n",
    "            articles_by_year[year].extend(articles)\n",
    "            print(f\"  Found {len(articles)} articles for year {year}\")\n",
    "        else:\n",
    "            print(f\"  No articles found in {filename}\")\n",
    "    \n",
    "    if not articles_by_year:\n",
    "        print(\"No articles were successfully parsed.\")\n",
    "        return\n",
    "    \n",
    "    # Create separate CSV files for each year\n",
    "    all_dataframes = {}\n",
    "    \n",
    "    for year, articles in articles_by_year.items():\n",
    "        print(f\"\\nProcessing {len(articles)} articles for year {year}...\")\n",
    "        \n",
    "        # Create DataFrame for this year\n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        column_order = [\n",
    "            'source_file', 'title', 'author', 'publication_info', 'abstract', \n",
    "            'urls', 'links', 'publication_title', 'publication_date', 'publication_year',\n",
    "            'section', 'pages', 'publisher', 'place_of_publication', 'issn',\n",
    "            'document_type', 'proquest_id', 'database', 'company_match', \n",
    "            'title_match', 'publication_subject_match', 'full_text'\n",
    "        ]\n",
    "        \n",
    "        # Reorder columns (only include columns that exist)\n",
    "        existing_columns = [col for col in column_order if col in df.columns]\n",
    "        df = df[existing_columns]\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = os.path.join(output_folder, f\"newspaper_data_{year}.csv\")\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Store dataframe for summary\n",
    "        all_dataframes[year] = df\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024**2)\n",
    "        print(f\"Saved {output_file}: {len(df)} articles, {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Show preview for this year\n",
    "        if len(df) > 0:\n",
    "            print(f\"Sample from {year}:\")\n",
    "            sample_cols = ['title', 'author', 'publication_title', 'publication_date']\n",
    "            available_cols = [col for col in sample_cols if col in df.columns]\n",
    "            print(df[available_cols].head(2))\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY STATISTICS BY YEAR\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total_articles = 0\n",
    "    for year in sorted(all_dataframes.keys()):\n",
    "        df = all_dataframes[year]\n",
    "        articles_count = len(df)\n",
    "        total_articles += articles_count\n",
    "        \n",
    "        abstracts_count = df['abstract'].notna().sum() if 'abstract' in df.columns else 0\n",
    "        unique_pubs = df['publication_title'].nunique() if 'publication_title' in df.columns else 0\n",
    "        \n",
    "        print(f\"Year {year}:\")\n",
    "        print(f\"  - Total articles: {articles_count}\")\n",
    "        print(f\"  - Articles with abstracts: {abstracts_count}\")\n",
    "        print(f\"  - Unique publications: {unique_pubs}\")\n",
    "        \n",
    "        if 'publication_date' in df.columns:\n",
    "            date_range = f\"{df['publication_date'].min()} to {df['publication_date'].max()}\"\n",
    "            print(f\"  - Date range: {date_range}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"TOTAL ARTICLES ACROSS ALL YEARS: {total_articles}\")\n",
    "    print(f\"CSV files created in: {output_folder}/\")\n",
    "    \n",
    "    return all_dataframes\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all files and create separate CSV files by year\n",
    "    dataframes_by_year = process_all_files_by_year()\n",
    "    \n",
    "    # Optional: Create a combined summary file with just key statistics\n",
    "    if dataframes_by_year:\n",
    "        summary_data = []\n",
    "        for year, df in dataframes_by_year.items():\n",
    "            summary_data.append({\n",
    "                'year': year,\n",
    "                'total_articles': len(df),\n",
    "                'articles_with_abstracts': df['abstract'].notna().sum() if 'abstract' in df.columns else 0,\n",
    "                'unique_publications': df['publication_title'].nunique() if 'publication_title' in df.columns else 0,\n",
    "                'unique_authors': df['author'].nunique() if 'author' in df.columns else 0\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(\"output_by_year/yearly_summary.csv\", index=False)\n",
    "        print(\"Created yearly summary file: output_by_year/yearly_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50de3a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publication_info</th>\n",
       "      <th>abstract</th>\n",
       "      <th>urls</th>\n",
       "      <th>links</th>\n",
       "      <th>publication_title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>...</th>\n",
       "      <th>publisher</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>issn</th>\n",
       "      <th>document_type</th>\n",
       "      <th>proquest_id</th>\n",
       "      <th>database</th>\n",
       "      <th>company_match</th>\n",
       "      <th>title_match</th>\n",
       "      <th>publication_subject_match</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024 1.txt</td>\n",
       "      <td>Kenny Dillingham makes Arizona State evolution...</td>\n",
       "      <td>Wolken, Dan</td>\n",
       "      <td>Wolken, Dan.</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://ezproxy.newcastle.edu.au/login?url=http...</td>\n",
       "      <td>https://newcastle.primo.exlibrisgroup.com/disc...</td>\n",
       "      <td>USA Today (Online); Arlington</td>\n",
       "      <td>Dec 31, 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>USA Today, a division of Gannett Satellite Inf...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>21651779</td>\n",
       "      <td>News</td>\n",
       "      <td>3150269804</td>\n",
       "      <td>U.S. Newsstream Collection</td>\n",
       "      <td>National Collegiate Athletic Association--NCAA...</td>\n",
       "      <td>Kenny Dillingham makes Arizona State evolution...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>ATLANTA — It all lined up perfectly, the 32-ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024 1.txt</td>\n",
       "      <td>What is the NCAA rushing record? Can Ashton Je...</td>\n",
       "      <td>Kassim, Ehsan</td>\n",
       "      <td>Kassim, Ehsan.</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://ezproxy.newcastle.edu.au/login?url=http...</td>\n",
       "      <td>https://newcastle.primo.exlibrisgroup.com/disc...</td>\n",
       "      <td>USA Today (Online); Arlington</td>\n",
       "      <td>Dec 31, 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>USA Today, a division of Gannett Satellite Inf...</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>21651779</td>\n",
       "      <td>News</td>\n",
       "      <td>3150269789</td>\n",
       "      <td>U.S. Newsstream Collection</td>\n",
       "      <td>National Collegiate Athletic Association--NCAA...</td>\n",
       "      <td>What is the NCAA rushing record? Can Ashton Je...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>Ashton Jeanty came up just short in his quest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024 1.txt</td>\n",
       "      <td>RED STORM NEED A SIGNATURE WIN: BEATING BLUEJA...</td>\n",
       "      <td>Rubin, Roger</td>\n",
       "      <td>Rubin, Roger.</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://ezproxy.newcastle.edu.au/login?url=http...</td>\n",
       "      <td>https://newcastle.primo.exlibrisgroup.com/disc...</td>\n",
       "      <td>Newsday, Combined editions; Long Island, N.Y.</td>\n",
       "      <td>Dec 31, 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>Newsday LLC</td>\n",
       "      <td>Long Island, N.Y.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>3150216422</td>\n",
       "      <td>U.S. Newsstream Collection</td>\n",
       "      <td>National Collegiate Athletic Association--NCAA...</td>\n",
       "      <td>RED STORM NEED A SIGNATURE WIN:   Beating Blue...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>Roger Rubin\\nroger.rubin@newsday.com OMAHA, Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024 1.txt</td>\n",
       "      <td>WIN trumps all: Boise State's Jeanty focused o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://ezproxy.newcastle.edu.au/login?url=http...</td>\n",
       "      <td>https://newcastle.primo.exlibrisgroup.com/disc...</td>\n",
       "      <td>Arizona Republic; Phoenix, Ariz.</td>\n",
       "      <td>Dec 31, 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>Gannett Media Corp</td>\n",
       "      <td>Phoenix, Ariz.</td>\n",
       "      <td>08928711</td>\n",
       "      <td>News</td>\n",
       "      <td>3150206898</td>\n",
       "      <td>U.S. Newsstream Collection</td>\n",
       "      <td>National Collegiate Athletic Association--NCAA...</td>\n",
       "      <td>WIN trumps all:   Boise State's Jeanty focused...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>Nobody could ever really stop Barry Sanders, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024 1.txt</td>\n",
       "      <td>SPORTS; Will this Duck be like Bo or Deion? Or...</td>\n",
       "      <td>De Leon, Anthony</td>\n",
       "      <td>De Leon, Anthony.</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://ezproxy.newcastle.edu.au/login?url=http...</td>\n",
       "      <td>https://newcastle.primo.exlibrisgroup.com/disc...</td>\n",
       "      <td>Los Angeles Times; Los Angeles, Calif.</td>\n",
       "      <td>Dec 31, 2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>Los Angeles Times Communications LLC</td>\n",
       "      <td>Los Angeles, Calif.</td>\n",
       "      <td>04583035</td>\n",
       "      <td>News</td>\n",
       "      <td>3150203937</td>\n",
       "      <td>U.S. Newsstream Collection</td>\n",
       "      <td>National Collegiate Athletic Association--NCAA...</td>\n",
       "      <td>SPORTS; Will this Duck be like Bo or Deion? Or...</td>\n",
       "      <td>General Interest Periodicals--United States</td>\n",
       "      <td>Oregon's Bryce Boettcher has the potential to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_file                                              title  \\\n",
       "0  2024 1.txt  Kenny Dillingham makes Arizona State evolution...   \n",
       "1  2024 1.txt  What is the NCAA rushing record? Can Ashton Je...   \n",
       "2  2024 1.txt  RED STORM NEED A SIGNATURE WIN: BEATING BLUEJA...   \n",
       "3  2024 1.txt  WIN trumps all: Boise State's Jeanty focused o...   \n",
       "4  2024 1.txt  SPORTS; Will this Duck be like Bo or Deion? Or...   \n",
       "\n",
       "             author   publication_info         abstract  \\\n",
       "0       Wolken, Dan       Wolken, Dan.  None available.   \n",
       "1     Kassim, Ehsan     Kassim, Ehsan.  None available.   \n",
       "2      Rubin, Roger      Rubin, Roger.  None available.   \n",
       "3               NaN                NaN  None available.   \n",
       "4  De Leon, Anthony  De Leon, Anthony.  None available.   \n",
       "\n",
       "                                                urls  \\\n",
       "0  http://ezproxy.newcastle.edu.au/login?url=http...   \n",
       "1  http://ezproxy.newcastle.edu.au/login?url=http...   \n",
       "2  http://ezproxy.newcastle.edu.au/login?url=http...   \n",
       "3  http://ezproxy.newcastle.edu.au/login?url=http...   \n",
       "4  http://ezproxy.newcastle.edu.au/login?url=http...   \n",
       "\n",
       "                                               links  \\\n",
       "0  https://newcastle.primo.exlibrisgroup.com/disc...   \n",
       "1  https://newcastle.primo.exlibrisgroup.com/disc...   \n",
       "2  https://newcastle.primo.exlibrisgroup.com/disc...   \n",
       "3  https://newcastle.primo.exlibrisgroup.com/disc...   \n",
       "4  https://newcastle.primo.exlibrisgroup.com/disc...   \n",
       "\n",
       "                               publication_title publication_date  \\\n",
       "0                  USA Today (Online); Arlington     Dec 31, 2024   \n",
       "1                  USA Today (Online); Arlington     Dec 31, 2024   \n",
       "2  Newsday, Combined editions; Long Island, N.Y.     Dec 31, 2024   \n",
       "3               Arizona Republic; Phoenix, Ariz.     Dec 31, 2024   \n",
       "4         Los Angeles Times; Los Angeles, Calif.     Dec 31, 2024   \n",
       "\n",
       "  publication_year  ...                                          publisher  \\\n",
       "0             2024  ...  USA Today, a division of Gannett Satellite Inf...   \n",
       "1             2024  ...  USA Today, a division of Gannett Satellite Inf...   \n",
       "2             2024  ...                                        Newsday LLC   \n",
       "3             2024  ...                                 Gannett Media Corp   \n",
       "4             2024  ...               Los Angeles Times Communications LLC   \n",
       "\n",
       "  place_of_publication      issn document_type proquest_id  \\\n",
       "0            Arlington  21651779          News  3150269804   \n",
       "1            Arlington  21651779          News  3150269789   \n",
       "2    Long Island, N.Y.       NaN          News  3150216422   \n",
       "3       Phoenix, Ariz.  08928711          News  3150206898   \n",
       "4  Los Angeles, Calif.  04583035          News  3150203937   \n",
       "\n",
       "                     database  \\\n",
       "0  U.S. Newsstream Collection   \n",
       "1  U.S. Newsstream Collection   \n",
       "2  U.S. Newsstream Collection   \n",
       "3  U.S. Newsstream Collection   \n",
       "4  U.S. Newsstream Collection   \n",
       "\n",
       "                                       company_match  \\\n",
       "0  National Collegiate Athletic Association--NCAA...   \n",
       "1  National Collegiate Athletic Association--NCAA...   \n",
       "2  National Collegiate Athletic Association--NCAA...   \n",
       "3  National Collegiate Athletic Association--NCAA...   \n",
       "4  National Collegiate Athletic Association--NCAA...   \n",
       "\n",
       "                                         title_match  \\\n",
       "0  Kenny Dillingham makes Arizona State evolution...   \n",
       "1  What is the NCAA rushing record? Can Ashton Je...   \n",
       "2  RED STORM NEED A SIGNATURE WIN:   Beating Blue...   \n",
       "3  WIN trumps all:   Boise State's Jeanty focused...   \n",
       "4  SPORTS; Will this Duck be like Bo or Deion? Or...   \n",
       "\n",
       "                     publication_subject_match  \\\n",
       "0  General Interest Periodicals--United States   \n",
       "1  General Interest Periodicals--United States   \n",
       "2  General Interest Periodicals--United States   \n",
       "3  General Interest Periodicals--United States   \n",
       "4  General Interest Periodicals--United States   \n",
       "\n",
       "                                           full_text  \n",
       "0  ATLANTA — It all lined up perfectly, the 32-ye...  \n",
       "1  Ashton Jeanty came up just short in his quest ...  \n",
       "2  Roger Rubin\\nroger.rubin@newsday.com OMAHA, Ne...  \n",
       "3  Nobody could ever really stop Barry Sanders, b...  \n",
       "4  Oregon's Bryce Boettcher has the potential to ...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"output_by_year/newspaper_data_2024.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897dbe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSV files with essential data only...\n",
      "Found 24 CSV files to combine...\n",
      "Processing newspaper_data_2002.csv...\n",
      "  Added 10159 articles from 2002\n",
      "Processing newspaper_data_2003.csv...\n",
      "  Added 11340 articles from 2003\n",
      "Processing newspaper_data_2004.csv...\n",
      "  Added 10850 articles from 2004\n",
      "Processing newspaper_data_2005.csv...\n",
      "  Added 10572 articles from 2005\n",
      "Processing newspaper_data_2006.csv...\n",
      "  Added 9805 articles from 2006\n",
      "Processing newspaper_data_2007.csv...\n",
      "  Added 8679 articles from 2007\n",
      "Processing newspaper_data_2008.csv...\n",
      "  Added 8490 articles from 2008\n",
      "Processing newspaper_data_2009.csv...\n",
      "  Added 8023 articles from 2009\n",
      "Processing newspaper_data_2010.csv...\n",
      "  Added 7978 articles from 2010\n",
      "Processing newspaper_data_2011.csv...\n",
      "  Added 7454 articles from 2011\n",
      "Processing newspaper_data_2012.csv...\n",
      "  Added 7717 articles from 2012\n",
      "Processing newspaper_data_2013.csv...\n",
      "  Added 6544 articles from 2013\n",
      "Processing newspaper_data_2014.csv...\n",
      "  Added 6032 articles from 2014\n",
      "Processing newspaper_data_2015.csv...\n",
      "  Added 6148 articles from 2015\n",
      "Processing newspaper_data_2016.csv...\n",
      "  Added 6030 articles from 2016\n",
      "Processing newspaper_data_2017.csv...\n",
      "  Added 5637 articles from 2017\n",
      "Processing newspaper_data_2018.csv...\n",
      "  Added 5683 articles from 2018\n",
      "Processing newspaper_data_2019.csv...\n",
      "  Added 6252 articles from 2019\n",
      "Processing newspaper_data_2020.csv...\n",
      "  Added 5042 articles from 2020\n",
      "Processing newspaper_data_2021.csv...\n",
      "  Added 5345 articles from 2021\n",
      "Processing newspaper_data_2022.csv...\n",
      "  Added 4640 articles from 2022\n",
      "Processing newspaper_data_2023.csv...\n",
      "  Added 5028 articles from 2023\n",
      "Processing newspaper_data_2024.csv...\n",
      "  Added 6392 articles from 2024\n",
      "Processing newspaper_data_2025.csv...\n",
      "  Added 4159 articles from 2025\n",
      "\n",
      "Combining all data...\n",
      "Cleaning data...\n",
      "\n",
      "==================================================\n",
      "ESSENTIAL DATA SUMMARY\n",
      "==================================================\n",
      "Total articles: 173999\n",
      "Years covered: 64 unique years\n",
      "Year range: 20 02 to 2025\n",
      "Unique publications: 257\n",
      "\n",
      "Articles per year:\n",
      "  20 02: 4 articles\n",
      "  20 03: 1 articles\n",
      "  20 04: 2 articles\n",
      "  20 06: 4 articles\n",
      "  20 07: 4 articles\n",
      "  20 09: 1 articles\n",
      "  20 10: 1 articles\n",
      "  20 11: 1 articles\n",
      "  20 12: 1 articles\n",
      "  20 13: 1 articles\n",
      "  20 16: 2 articles\n",
      "  20 17: 3 articles\n",
      "  20 18: 2 articles\n",
      "  20 19: 5 articles\n",
      "  20 21: 8 articles\n",
      "  20 23: 4 articles\n",
      "  20 24: 1 articles\n",
      "  20 25: 2 articles\n",
      "  200 2: 3 articles\n",
      "  200 3: 6 articles\n",
      "  200 4: 2 articles\n",
      "  200 5: 3 articles\n",
      "  200 6: 2 articles\n",
      "  200 7: 2 articles\n",
      "  2002: 10152 articles\n",
      "  2003: 11333 articles\n",
      "  2004: 10846 articles\n",
      "  2005: 10569 articles\n",
      "  2006: 9799 articles\n",
      "  2007: 8673 articles\n",
      "  2008: 12 articles\n",
      "  2008.0: 8478 articles\n",
      "  2009: 8022 articles\n",
      "  201 0: 4 articles\n",
      "  201 1: 2 articles\n",
      "  201 2: 1 articles\n",
      "  201 4: 4 articles\n",
      "  201 5: 1 articles\n",
      "  201 6: 1 articles\n",
      "  201 7: 3 articles\n",
      "  201 8: 2 articles\n",
      "  201 9: 3 articles\n",
      "  2010: 7973 articles\n",
      "  2011: 7451 articles\n",
      "  2012: 7715 articles\n",
      "  2013: 6543 articles\n",
      "  2014: 6028 articles\n",
      "  2015: 6147 articles\n",
      "  2016: 6027 articles\n",
      "  2017: 5631 articles\n",
      "  2018: 5679 articles\n",
      "  2019: 6244 articles\n",
      "  202 0: 4 articles\n",
      "  202 1: 3 articles\n",
      "  202 2: 4 articles\n",
      "  202 3: 4 articles\n",
      "  202 4: 4 articles\n",
      "  202 5: 4 articles\n",
      "  2020: 5038 articles\n",
      "  2021: 5334 articles\n",
      "  2022: 4636 articles\n",
      "  2023: 5020 articles\n",
      "  2024: 6387 articles\n",
      "  2025: 4153 articles\n",
      "\n",
      "Top 10 publications:\n",
      "  Chicago Tribune; Chicago, Ill.: 19852 articles\n",
      "  The Washington Post; Washington, D.C.: 19487 articles\n",
      "  Los Angeles Times; Los Angeles, Calif.: 17182 articles\n",
      "  Boston Globe; Boston, Mass.: 16614 articles\n",
      "  USA TODAY; McLean, Va.: 13455 articles\n",
      "  Philadelphia Inquirer; Philadelphia, Pa.: 13411 articles\n",
      "  Star Tribune; Minneapolis, Minn.: 12638 articles\n",
      "  Arizona Republic; Phoenix, Ariz.: 11132 articles\n",
      "  Newsday, Combined editions; Long Island, N.Y.: 10742 articles\n",
      "  USA Today (Online); Arlington: 10443 articles\n",
      "\n",
      "Data quality:\n",
      "  Articles with titles: 173999\n",
      "  Articles with main text: 173999\n",
      "  Articles with publication name: 173999\n",
      "\n",
      "Output file: essential_newspaper_data.csv\n",
      "File size: 750.77 MB\n",
      "\n",
      "Sample data (first 3 rows):\n",
      " year                                                                                                                               title                              publication_name\n",
      "20 02 A Sudden Tragedy, a Long Recovery; After Illness and Amputations, Virginia Tech's DuBose Discovers a World of Help: [FINAL Edition]         The Washington Post; Washington, D.C.\n",
      "20 02                         Down-home coach leads La. Tech ; Barmore's program short on funds but not short on success: [FINAL Edition]                        USA TODAY; McLean, Va.\n",
      "20 02                                                                  It's a Must for Latrell to Play Well - Next Season: [ALL EDITIONS] Newsday, Combined editions; Long Island, N.Y.\n",
      "\n",
      "File is 750.77 MB - creating compact version...\n",
      "\n",
      "Creating compact version...\n",
      "Text length statistics:\n",
      "  Average title length: 66 characters\n",
      "  Average main text length: 4377 characters\n",
      "  Max title length: 1061\n",
      "  Max main text length: 97729\n",
      "  Truncating 1760 very long titles\n",
      "  Truncating 51709 very long texts\n",
      "Original file: 750.77 MB\n",
      "Compact file: 628.01 MB\n",
      "Size reduction: 16.4%\n",
      "\n",
      "Done! Your essential data is ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def combine_essential_data(input_folder=\"output_by_year\", output_file=\"essential_newspaper_data.csv\"):\n",
    "    \"\"\"\n",
    "    Combine all year-based CSV files and keep only essential columns:\n",
    "    - Year of publication\n",
    "    - Date of publication\n",
    "    - Title\n",
    "    - Main text (full_text)\n",
    "    - Name of publication (publication_title)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Combining CSV files with essential data only...\")\n",
    "    \n",
    "    # Get all CSV files (excluding summary file)\n",
    "    csv_pattern = os.path.join(input_folder, \"newspaper_data_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found matching pattern: {csv_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to combine...\")\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for csv_file in sorted(csv_files):  # Sort to process years in order\n",
    "        print(f\"Processing {os.path.basename(csv_file)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Extract year from filename as backup\n",
    "            filename = os.path.basename(csv_file)\n",
    "            year_from_filename = filename.replace(\"newspaper_data_\", \"\").replace(\".csv\", \"\")\n",
    "            \n",
    "            # Create essential data dictionary\n",
    "            essential_cols = {\n",
    "                'year': 'publication_year',\n",
    "                'date': 'publication_date',\n",
    "                'title': 'title', \n",
    "                'main_text': 'full_text',\n",
    "                'publication_name': 'publication_title'\n",
    "            }\n",
    "            \n",
    "            # Check which columns exist in the dataframe\n",
    "            available_data = {}\n",
    "            for new_name, orig_name in essential_cols.items():\n",
    "                if orig_name in df.columns:\n",
    "                    available_data[new_name] = df[orig_name]\n",
    "                else:\n",
    "                    print(f\"  Warning: Column '{orig_name}' not found in {filename}\")\n",
    "                    available_data[new_name] = ''  # Empty string if column missing\n",
    "            \n",
    "            # Use year from filename if publication_year is missing/empty\n",
    "            if 'year' in available_data:\n",
    "                # Fill missing years with year from filename\n",
    "                available_data['year'] = available_data['year'].fillna(year_from_filename)\n",
    "                available_data['year'] = available_data['year'].replace('', year_from_filename)\n",
    "            \n",
    "            # Create DataFrame with essential columns\n",
    "            essential_df = pd.DataFrame(available_data)\n",
    "            \n",
    "            # Add to combined data\n",
    "            combined_data.append(essential_df)\n",
    "            \n",
    "            print(f\"  Added {len(essential_df)} articles from {year_from_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not combined_data:\n",
    "        print(\"No data was successfully processed.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    print(\"\\nCombining all data...\")\n",
    "    final_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    # Clean up the data\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    # Remove rows where all essential fields are empty\n",
    "    essential_fields = ['title', 'main_text', 'publication_name']\n",
    "    mask = final_df[essential_fields].notna().any(axis=1)\n",
    "    final_df = final_df[mask]\n",
    "    \n",
    "    # Convert year to string and clean it\n",
    "    final_df['year'] = final_df['year'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove extra whitespace from text fields\n",
    "    text_columns = ['title', 'main_text', 'publication_name']\n",
    "    for col in text_columns:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].astype(str).str.strip()\n",
    "    \n",
    "    # Sort by year and title\n",
    "    final_df = final_df.sort_values(['year', 'title'], na_position='last')\n",
    "    \n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"ESSENTIAL DATA SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total articles: {len(final_df)}\")\n",
    "    print(f\"Years covered: {final_df['year'].nunique()} unique years\")\n",
    "    print(f\"Year range: {final_df['year'].min()} to {final_df['year'].max()}\")\n",
    "    print(f\"Unique publications: {final_df['publication_name'].nunique()}\")\n",
    "    \n",
    "    # Show articles per year\n",
    "    print(f\"\\nArticles per year:\")\n",
    "    year_counts = final_df['year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} articles\")\n",
    "    \n",
    "    # Show top publications\n",
    "    print(f\"\\nTop 10 publications:\")\n",
    "    pub_counts = final_df['publication_name'].value_counts().head(10)\n",
    "    for pub, count in pub_counts.items():\n",
    "        print(f\"  {pub}: {count} articles\")\n",
    "    \n",
    "    # Show data quality stats\n",
    "    print(f\"\\nData quality:\")\n",
    "    print(f\"  Articles with titles: {final_df['title'].notna().sum()}\")\n",
    "    print(f\"  Articles with main text: {final_df['main_text'].notna().sum()}\")\n",
    "    print(f\"  Articles with publication name: {final_df['publication_name'].notna().sum()}\")\n",
    "    \n",
    "    # Show file size\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024**2)\n",
    "    print(f\"\\nOutput file: {output_file}\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nSample data (first 3 rows):\")\n",
    "    sample_df = final_df[['year', 'title', 'publication_name']].head(3)\n",
    "    print(sample_df.to_string(index=False))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_even_more_compact_version(df, output_file=\"compact_newspaper_data.csv\"):\n",
    "    \"\"\"Create an even more compact version by removing very long texts\"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating compact version...\")\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    df['title_length'] = df['title'].astype(str).str.len()\n",
    "    df['text_length'] = df['main_text'].astype(str).str.len()\n",
    "    \n",
    "    print(f\"Text length statistics:\")\n",
    "    print(f\"  Average title length: {df['title_length'].mean():.0f} characters\")\n",
    "    print(f\"  Average main text length: {df['text_length'].mean():.0f} characters\")\n",
    "    print(f\"  Max title length: {df['title_length'].max()}\")\n",
    "    print(f\"  Max main text length: {df['text_length'].max()}\")\n",
    "    \n",
    "    # Create compact version - you can adjust these limits\n",
    "    MAX_TITLE_LENGTH = 200  # Characters\n",
    "    MAX_TEXT_LENGTH = 5000  # Characters (about 1000 words)\n",
    "    \n",
    "    compact_df = df.copy()\n",
    "    \n",
    "    # Truncate very long titles\n",
    "    long_titles = compact_df['title_length'] > MAX_TITLE_LENGTH\n",
    "    if long_titles.sum() > 0:\n",
    "        print(f\"  Truncating {long_titles.sum()} very long titles\")\n",
    "        compact_df.loc[long_titles, 'title'] = compact_df.loc[long_titles, 'title'].str[:MAX_TITLE_LENGTH] + '...'\n",
    "    \n",
    "    # Truncate very long texts\n",
    "    long_texts = compact_df['text_length'] > MAX_TEXT_LENGTH\n",
    "    if long_texts.sum() > 0:\n",
    "        print(f\"  Truncating {long_texts.sum()} very long texts\")\n",
    "        compact_df.loc[long_texts, 'main_text'] = compact_df.loc[long_texts, 'main_text'].str[:MAX_TEXT_LENGTH] + '...'\n",
    "    \n",
    "    # Remove length columns\n",
    "    compact_df = compact_df.drop(['title_length', 'text_length'], axis=1)\n",
    "    \n",
    "    # Save compact version\n",
    "    compact_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Compare file sizes\n",
    "    original_size = os.path.getsize(\"essential_newspaper_data.csv\") / (1024**2)\n",
    "    compact_size = os.path.getsize(output_file) / (1024**2)\n",
    "    \n",
    "    print(f\"Original file: {original_size:.2f} MB\")\n",
    "    print(f\"Compact file: {compact_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {100*(1-compact_size/original_size):.1f}%\")\n",
    "    \n",
    "    return compact_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Combine essential data\n",
    "    df = combine_essential_data(\n",
    "        input_folder=\"output_by_year\", \n",
    "        output_file=\"essential_newspaper_data.csv\"\n",
    "    )\n",
    "    \n",
    "    if df is not None:\n",
    "        # Optional: Create compact version if file is still too large\n",
    "        file_size_mb = os.path.getsize(\"essential_newspaper_data.csv\") / (1024**2)\n",
    "        if file_size_mb > 100:  # If larger than 100MB\n",
    "            print(f\"\\nFile is {file_size_mb:.2f} MB - creating compact version...\")\n",
    "            compact_df = create_even_more_compact_version(df, \"compact_newspaper_data.csv\")\n",
    "        else:\n",
    "            print(f\"\\nFile size ({file_size_mb:.2f} MB) is manageable.\")\n",
    "        \n",
    "        print(f\"\\nDone! Your essential data is ready for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90fb3d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\rafsu\\OneDrive - The University Of Newcastle\\NCAA_Alex\\complete_data_raw\\codes\n",
      "\n",
      "Looking for CSV files...\n",
      "\n",
      "In '.':\n",
      "  All files: ['cleaning_report.txt', 'compact_newspaper_data.csv', 'data_cleaning.ipynb', 'data_loading.ipynb', 'essential_newspaper_data.csv', 'newspaper_data.csv', 'output_by_year', 'text_normalizer.py']...\n",
      "  CSV files: ['compact_newspaper_data.csv', 'essential_newspaper_data.csv', 'newspaper_data.csv']\n",
      "\n",
      "In 'output_by_year':\n",
      "  All files: ['newspaper_data_2002.csv', 'newspaper_data_2003.csv', 'newspaper_data_2004.csv', 'newspaper_data_2005.csv', 'newspaper_data_2006.csv', 'newspaper_data_2007.csv', 'newspaper_data_2008.csv', 'newspaper_data_2009.csv', 'newspaper_data_2010.csv', 'newspaper_data_2011.csv']...\n",
      "  CSV files: ['newspaper_data_2002.csv', 'newspaper_data_2003.csv', 'newspaper_data_2004.csv', 'newspaper_data_2005.csv', 'newspaper_data_2006.csv', 'newspaper_data_2007.csv', 'newspaper_data_2008.csv', 'newspaper_data_2009.csv', 'newspaper_data_2010.csv', 'newspaper_data_2011.csv', 'newspaper_data_2012.csv', 'newspaper_data_2013.csv', 'newspaper_data_2014.csv', 'newspaper_data_2015.csv', 'newspaper_data_2016.csv', 'newspaper_data_2017.csv', 'newspaper_data_2018.csv', 'newspaper_data_2019.csv', 'newspaper_data_2020.csv', 'newspaper_data_2021.csv', 'newspaper_data_2022.csv', 'newspaper_data_2023.csv', 'newspaper_data_2024.csv', 'newspaper_data_2025.csv', 'yearly_summary.csv']\n",
      "\n",
      "'data' does not exist\n",
      "\n",
      "In '../data':\n",
      "  All files: ['cleaned_newspaper_data.csv', 'NCAA_Newspaper_articles', 'normalized_newspaper_data.csv', 'normalized_newspaper_data.xlsx', 'output_by_year']...\n",
      "  CSV files: ['cleaned_newspaper_data.csv', 'normalized_newspaper_data.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check current directory and look for CSV files\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"\\nLooking for CSV files...\")\n",
    "\n",
    "# Check common locations\n",
    "locations_to_check = [\n",
    "    \".\",  # Current directory\n",
    "    \"output_by_year\",\n",
    "    \"data\",\n",
    "    \"../data\"\n",
    "]\n",
    "\n",
    "for location in locations_to_check:\n",
    "    if os.path.exists(location):\n",
    "        files = os.listdir(location)\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        print(f\"\\nIn '{location}':\")\n",
    "        print(f\"  All files: {files[:10]}...\")  # Show first 10 files\n",
    "        print(f\"  CSV files: {csv_files}\")\n",
    "    else:\n",
    "        print(f\"\\n'{location}' does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19977ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in output_by_year:\n",
      "  newspaper_data_2002.csv\n",
      "  newspaper_data_2003.csv\n",
      "  newspaper_data_2004.csv\n",
      "  newspaper_data_2005.csv\n",
      "  newspaper_data_2006.csv\n",
      "  newspaper_data_2007.csv\n",
      "  newspaper_data_2008.csv\n",
      "  newspaper_data_2009.csv\n",
      "  newspaper_data_2010.csv\n",
      "  newspaper_data_2011.csv\n",
      "  newspaper_data_2012.csv\n",
      "  newspaper_data_2013.csv\n",
      "  newspaper_data_2014.csv\n",
      "  newspaper_data_2015.csv\n",
      "  newspaper_data_2016.csv\n",
      "  newspaper_data_2017.csv\n",
      "  newspaper_data_2018.csv\n",
      "  newspaper_data_2019.csv\n",
      "  newspaper_data_2020.csv\n",
      "  newspaper_data_2021.csv\n",
      "  newspaper_data_2022.csv\n",
      "  newspaper_data_2023.csv\n",
      "  newspaper_data_2024.csv\n",
      "  newspaper_data_2025.csv\n",
      "  yearly_summary.csv\n",
      "\n",
      "CSV files found: 25\n",
      "  newspaper_data_2002.csv\n",
      "  newspaper_data_2003.csv\n",
      "  newspaper_data_2004.csv\n",
      "  newspaper_data_2005.csv\n",
      "  newspaper_data_2006.csv\n",
      "  newspaper_data_2007.csv\n",
      "  newspaper_data_2008.csv\n",
      "  newspaper_data_2009.csv\n",
      "  newspaper_data_2010.csv\n",
      "  newspaper_data_2011.csv\n",
      "  newspaper_data_2012.csv\n",
      "  newspaper_data_2013.csv\n",
      "  newspaper_data_2014.csv\n",
      "  newspaper_data_2015.csv\n",
      "  newspaper_data_2016.csv\n",
      "  newspaper_data_2017.csv\n",
      "  newspaper_data_2018.csv\n",
      "  newspaper_data_2019.csv\n",
      "  newspaper_data_2020.csv\n",
      "  newspaper_data_2021.csv\n",
      "  newspaper_data_2022.csv\n",
      "  newspaper_data_2023.csv\n",
      "  newspaper_data_2024.csv\n",
      "  newspaper_data_2025.csv\n",
      "  yearly_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check what's in the output_by_year folder\n",
    "folder_path = \"output_by_year\"\n",
    "if os.path.exists(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    print(f\"Files in {folder_path}:\")\n",
    "    for file in files:\n",
    "        print(f\"  {file}\")\n",
    "        \n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "    print(f\"\\nCSV files found: {len(csv_files)}\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  {csv_file}\")\n",
    "else:\n",
    "    print(f\"Folder {folder_path} does not exist\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5987977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSV files with essential data only...\n",
      "Files in 'output_by_year': ['newspaper_data_2002.csv', 'newspaper_data_2003.csv', 'newspaper_data_2004.csv', 'newspaper_data_2005.csv', 'newspaper_data_2006.csv', 'newspaper_data_2007.csv', 'newspaper_data_2008.csv', 'newspaper_data_2009.csv', 'newspaper_data_2010.csv', 'newspaper_data_2011.csv', 'newspaper_data_2012.csv', 'newspaper_data_2013.csv', 'newspaper_data_2014.csv', 'newspaper_data_2015.csv', 'newspaper_data_2016.csv', 'newspaper_data_2017.csv', 'newspaper_data_2018.csv', 'newspaper_data_2019.csv', 'newspaper_data_2020.csv', 'newspaper_data_2021.csv', 'newspaper_data_2022.csv', 'newspaper_data_2023.csv', 'newspaper_data_2024.csv', 'newspaper_data_2025.csv', 'yearly_summary.csv']\n",
      "Found 24 files matching pattern 'newspaper_data_*.csv'\n",
      "Found 24 CSV files to combine...\n",
      "Processing newspaper_data_2002.csv...\n",
      "  Added 10159 articles from 2002\n",
      "Processing newspaper_data_2003.csv...\n",
      "  Added 11340 articles from 2003\n",
      "Processing newspaper_data_2004.csv...\n",
      "  Added 10850 articles from 2004\n",
      "Processing newspaper_data_2005.csv...\n",
      "  Added 10572 articles from 2005\n",
      "Processing newspaper_data_2006.csv...\n",
      "  Added 9805 articles from 2006\n",
      "Processing newspaper_data_2007.csv...\n",
      "  Added 8679 articles from 2007\n",
      "Processing newspaper_data_2008.csv...\n",
      "  Added 8490 articles from 2008\n",
      "Processing newspaper_data_2009.csv...\n",
      "  Added 8023 articles from 2009\n",
      "Processing newspaper_data_2010.csv...\n",
      "  Added 7978 articles from 2010\n",
      "Processing newspaper_data_2011.csv...\n",
      "  Added 7454 articles from 2011\n",
      "Processing newspaper_data_2012.csv...\n",
      "  Added 7717 articles from 2012\n",
      "Processing newspaper_data_2013.csv...\n",
      "  Added 6544 articles from 2013\n",
      "Processing newspaper_data_2014.csv...\n",
      "  Added 6032 articles from 2014\n",
      "Processing newspaper_data_2015.csv...\n",
      "  Added 6148 articles from 2015\n",
      "Processing newspaper_data_2016.csv...\n",
      "  Added 6030 articles from 2016\n",
      "Processing newspaper_data_2017.csv...\n",
      "  Added 5637 articles from 2017\n",
      "Processing newspaper_data_2018.csv...\n",
      "  Added 5683 articles from 2018\n",
      "Processing newspaper_data_2019.csv...\n",
      "  Added 6252 articles from 2019\n",
      "Processing newspaper_data_2020.csv...\n",
      "  Added 5042 articles from 2020\n",
      "Processing newspaper_data_2021.csv...\n",
      "  Added 5345 articles from 2021\n",
      "Processing newspaper_data_2022.csv...\n",
      "  Added 4640 articles from 2022\n",
      "Processing newspaper_data_2023.csv...\n",
      "  Added 5028 articles from 2023\n",
      "Processing newspaper_data_2024.csv...\n",
      "  Added 6392 articles from 2024\n",
      "Processing newspaper_data_2025.csv...\n",
      "  Added 4159 articles from 2025\n",
      "\n",
      "Combining all data...\n",
      "Cleaning data...\n",
      "\n",
      "==================================================\n",
      "ESSENTIAL DATA SUMMARY\n",
      "==================================================\n",
      "Total articles: 173999\n",
      "Years covered: 64 unique years\n",
      "Year range: 20 02 to 2025\n",
      "Unique publications: 257\n",
      "\n",
      "Articles per year:\n",
      "  20 02: 4 articles\n",
      "  20 03: 1 articles\n",
      "  20 04: 2 articles\n",
      "  20 06: 4 articles\n",
      "  20 07: 4 articles\n",
      "  20 09: 1 articles\n",
      "  20 10: 1 articles\n",
      "  20 11: 1 articles\n",
      "  20 12: 1 articles\n",
      "  20 13: 1 articles\n",
      "  20 16: 2 articles\n",
      "  20 17: 3 articles\n",
      "  20 18: 2 articles\n",
      "  20 19: 5 articles\n",
      "  20 21: 8 articles\n",
      "  20 23: 4 articles\n",
      "  20 24: 1 articles\n",
      "  20 25: 2 articles\n",
      "  200 2: 3 articles\n",
      "  200 3: 6 articles\n",
      "  200 4: 2 articles\n",
      "  200 5: 3 articles\n",
      "  200 6: 2 articles\n",
      "  200 7: 2 articles\n",
      "  2002: 10152 articles\n",
      "  2003: 11333 articles\n",
      "  2004: 10846 articles\n",
      "  2005: 10569 articles\n",
      "  2006: 9799 articles\n",
      "  2007: 8673 articles\n",
      "  2008: 12 articles\n",
      "  2008.0: 8478 articles\n",
      "  2009: 8022 articles\n",
      "  201 0: 4 articles\n",
      "  201 1: 2 articles\n",
      "  201 2: 1 articles\n",
      "  201 4: 4 articles\n",
      "  201 5: 1 articles\n",
      "  201 6: 1 articles\n",
      "  201 7: 3 articles\n",
      "  201 8: 2 articles\n",
      "  201 9: 3 articles\n",
      "  2010: 7973 articles\n",
      "  2011: 7451 articles\n",
      "  2012: 7715 articles\n",
      "  2013: 6543 articles\n",
      "  2014: 6028 articles\n",
      "  2015: 6147 articles\n",
      "  2016: 6027 articles\n",
      "  2017: 5631 articles\n",
      "  2018: 5679 articles\n",
      "  2019: 6244 articles\n",
      "  202 0: 4 articles\n",
      "  202 1: 3 articles\n",
      "  202 2: 4 articles\n",
      "  202 3: 4 articles\n",
      "  202 4: 4 articles\n",
      "  202 5: 4 articles\n",
      "  2020: 5038 articles\n",
      "  2021: 5334 articles\n",
      "  2022: 4636 articles\n",
      "  2023: 5020 articles\n",
      "  2024: 6387 articles\n",
      "  2025: 4153 articles\n",
      "\n",
      "Top 10 publications:\n",
      "  Chicago Tribune; Chicago, Ill.: 19852 articles\n",
      "  The Washington Post; Washington, D.C.: 19487 articles\n",
      "  Los Angeles Times; Los Angeles, Calif.: 17182 articles\n",
      "  Boston Globe; Boston, Mass.: 16614 articles\n",
      "  USA TODAY; McLean, Va.: 13455 articles\n",
      "  Philadelphia Inquirer; Philadelphia, Pa.: 13411 articles\n",
      "  Star Tribune; Minneapolis, Minn.: 12638 articles\n",
      "  Arizona Republic; Phoenix, Ariz.: 11132 articles\n",
      "  Newsday, Combined editions; Long Island, N.Y.: 10742 articles\n",
      "  USA Today (Online); Arlington: 10443 articles\n",
      "\n",
      "Data quality:\n",
      "  Articles with titles: 173999\n",
      "  Articles with main text: 173999\n",
      "  Articles with publication name: 173999\n",
      "\n",
      "Output file: essential_newspaper_data.csv\n",
      "File size: 750.77 MB\n",
      "\n",
      "Sample data (first 3 rows):\n",
      " year                                                                                                                               title                              publication_name\n",
      "20 02 A Sudden Tragedy, a Long Recovery; After Illness and Amputations, Virginia Tech's DuBose Discovers a World of Help: [FINAL Edition]         The Washington Post; Washington, D.C.\n",
      "20 02                         Down-home coach leads La. Tech ; Barmore's program short on funds but not short on success: [FINAL Edition]                        USA TODAY; McLean, Va.\n",
      "20 02                                                                  It's a Must for Latrell to Play Well - Next Season: [ALL EDITIONS] Newsday, Combined editions; Long Island, N.Y.\n",
      "\n",
      "File is 750.77 MB - creating compact version...\n",
      "\n",
      "Creating compact version...\n",
      "Text length statistics:\n",
      "  Average title length: 66 characters\n",
      "  Average main text length: 4377 characters\n",
      "  Max title length: 1061\n",
      "  Max main text length: 97729\n",
      "  Truncating 1760 very long titles\n",
      "  Truncating 51709 very long texts\n",
      "Original file: 750.77 MB\n",
      "Compact file: 628.01 MB\n",
      "Size reduction: 16.4%\n",
      "\n",
      "Done! Your essential data is ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def combine_essential_data(input_folder=\"output_by_year\", output_file=\"essential_newspaper_data.csv\"):\n",
    "    \"\"\"\n",
    "    Combine all year-based CSV files and keep only essential columns:\n",
    "    - Year of publication\n",
    "    - Date of publication\n",
    "    - Title\n",
    "    - Main text (full_text)\n",
    "    - Name of publication (publication_title)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Combining CSV files with essential data only...\")\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Folder '{input_folder}' does not exist!\")\n",
    "        print(\"Please provide the correct folder path where your year-based CSV files are located.\")\n",
    "        return None\n",
    "    \n",
    "    # List all files in the folder to debug\n",
    "    all_files = os.listdir(input_folder)\n",
    "    print(f\"Files in '{input_folder}': {all_files}\")\n",
    "    \n",
    "    # Try different patterns to find CSV files\n",
    "    patterns = [\n",
    "        \"newspaper_data_*.csv\",  # Original pattern\n",
    "        \"*_*.csv\",               # Any CSV with underscore\n",
    "        \"*.csv\"                  # Any CSV file\n",
    "    ]\n",
    "    \n",
    "    csv_files = []\n",
    "    for pattern in patterns:\n",
    "        csv_pattern = os.path.join(input_folder, pattern)\n",
    "        found_files = glob.glob(csv_pattern)\n",
    "        if found_files:\n",
    "            print(f\"Found {len(found_files)} files matching pattern '{pattern}'\")\n",
    "            csv_files = found_files\n",
    "            break\n",
    "    \n",
    "    # Exclude summary file if it exists\n",
    "    csv_files = [f for f in csv_files if 'summary' not in os.path.basename(f).lower()]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in '{input_folder}'\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"1. The folder path is correct\")\n",
    "        print(\"2. The CSV files are in this folder\")\n",
    "        print(\"3. The files have .csv extension\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to combine...\")\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for csv_file in sorted(csv_files):  # Sort to process years in order\n",
    "        print(f\"Processing {os.path.basename(csv_file)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Extract year from filename as backup\n",
    "            filename = os.path.basename(csv_file)\n",
    "            year_from_filename = filename.replace(\"newspaper_data_\", \"\").replace(\".csv\", \"\")\n",
    "            \n",
    "            # Create essential data dictionary\n",
    "            essential_cols = {\n",
    "                'year': 'publication_year',\n",
    "                'date': 'publication_date',\n",
    "                'title': 'title', \n",
    "                'main_text': 'full_text',\n",
    "                'publication_name': 'publication_title'\n",
    "            }\n",
    "            \n",
    "            # Check which columns exist in the dataframe\n",
    "            available_data = {}\n",
    "            for new_name, orig_name in essential_cols.items():\n",
    "                if orig_name in df.columns:\n",
    "                    available_data[new_name] = df[orig_name]\n",
    "                else:\n",
    "                    print(f\"  Warning: Column '{orig_name}' not found in {filename}\")\n",
    "                    available_data[new_name] = ''  # Empty string if column missing\n",
    "            \n",
    "            # Use year from filename if publication_year is missing/empty\n",
    "            if 'year' in available_data:\n",
    "                # Fill missing years with year from filename\n",
    "                available_data['year'] = available_data['year'].fillna(year_from_filename)\n",
    "                available_data['year'] = available_data['year'].replace('', year_from_filename)\n",
    "            \n",
    "            # Create DataFrame with essential columns\n",
    "            essential_df = pd.DataFrame(available_data)\n",
    "            \n",
    "            # Add to combined data\n",
    "            combined_data.append(essential_df)\n",
    "            \n",
    "            print(f\"  Added {len(essential_df)} articles from {year_from_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not combined_data:\n",
    "        print(\"No data was successfully processed.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    print(\"\\nCombining all data...\")\n",
    "    final_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    # Clean up the data\n",
    "    print(\"Cleaning data...\")\n",
    "    \n",
    "    # Remove rows where all essential fields are empty\n",
    "    essential_fields = ['title', 'main_text', 'publication_name']\n",
    "    mask = final_df[essential_fields].notna().any(axis=1)\n",
    "    final_df = final_df[mask]\n",
    "    \n",
    "    # Convert year to string and clean it\n",
    "    final_df['year'] = final_df['year'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove extra whitespace from text fields\n",
    "    text_columns = ['title', 'main_text', 'publication_name']\n",
    "    for col in text_columns:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].astype(str).str.strip()\n",
    "    \n",
    "    # Sort by year and title\n",
    "    final_df = final_df.sort_values(['year', 'title'], na_position='last')\n",
    "    \n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"ESSENTIAL DATA SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total articles: {len(final_df)}\")\n",
    "    print(f\"Years covered: {final_df['year'].nunique()} unique years\")\n",
    "    print(f\"Year range: {final_df['year'].min()} to {final_df['year'].max()}\")\n",
    "    print(f\"Unique publications: {final_df['publication_name'].nunique()}\")\n",
    "    \n",
    "    # Show articles per year\n",
    "    print(f\"\\nArticles per year:\")\n",
    "    year_counts = final_df['year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} articles\")\n",
    "    \n",
    "    # Show top publications\n",
    "    print(f\"\\nTop 10 publications:\")\n",
    "    pub_counts = final_df['publication_name'].value_counts().head(10)\n",
    "    for pub, count in pub_counts.items():\n",
    "        print(f\"  {pub}: {count} articles\")\n",
    "    \n",
    "    # Show data quality stats\n",
    "    print(f\"\\nData quality:\")\n",
    "    print(f\"  Articles with titles: {final_df['title'].notna().sum()}\")\n",
    "    print(f\"  Articles with main text: {final_df['main_text'].notna().sum()}\")\n",
    "    print(f\"  Articles with publication name: {final_df['publication_name'].notna().sum()}\")\n",
    "    \n",
    "    # Show file size\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024**2)\n",
    "    print(f\"\\nOutput file: {output_file}\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nSample data (first 3 rows):\")\n",
    "    sample_df = final_df[['year', 'title', 'publication_name']].head(3)\n",
    "    print(sample_df.to_string(index=False))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_even_more_compact_version(df, output_file=\"compact_newspaper_data.csv\"):\n",
    "    \"\"\"Create an even more compact version by removing very long texts\"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating compact version...\")\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    df['title_length'] = df['title'].astype(str).str.len()\n",
    "    df['text_length'] = df['main_text'].astype(str).str.len()\n",
    "    \n",
    "    print(f\"Text length statistics:\")\n",
    "    print(f\"  Average title length: {df['title_length'].mean():.0f} characters\")\n",
    "    print(f\"  Average main text length: {df['text_length'].mean():.0f} characters\")\n",
    "    print(f\"  Max title length: {df['title_length'].max()}\")\n",
    "    print(f\"  Max main text length: {df['text_length'].max()}\")\n",
    "    \n",
    "    # Create compact version - you can adjust these limits\n",
    "    MAX_TITLE_LENGTH = 200  # Characters\n",
    "    MAX_TEXT_LENGTH = 5000  # Characters (about 1000 words)\n",
    "    \n",
    "    compact_df = df.copy()\n",
    "    \n",
    "    # Truncate very long titles\n",
    "    long_titles = compact_df['title_length'] > MAX_TITLE_LENGTH\n",
    "    if long_titles.sum() > 0:\n",
    "        print(f\"  Truncating {long_titles.sum()} very long titles\")\n",
    "        compact_df.loc[long_titles, 'title'] = compact_df.loc[long_titles, 'title'].str[:MAX_TITLE_LENGTH] + '...'\n",
    "    \n",
    "    # Truncate very long texts\n",
    "    long_texts = compact_df['text_length'] > MAX_TEXT_LENGTH\n",
    "    if long_texts.sum() > 0:\n",
    "        print(f\"  Truncating {long_texts.sum()} very long texts\")\n",
    "        compact_df.loc[long_texts, 'main_text'] = compact_df.loc[long_texts, 'main_text'].str[:MAX_TEXT_LENGTH] + '...'\n",
    "    \n",
    "    # Remove length columns\n",
    "    compact_df = compact_df.drop(['title_length', 'text_length'], axis=1)\n",
    "    \n",
    "    # Save compact version\n",
    "    compact_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Compare file sizes\n",
    "    original_size = os.path.getsize(\"essential_newspaper_data.csv\") / (1024**2)\n",
    "    compact_size = os.path.getsize(output_file) / (1024**2)\n",
    "    \n",
    "    print(f\"Original file: {original_size:.2f} MB\")\n",
    "    print(f\"Compact file: {compact_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {100*(1-compact_size/original_size):.1f}%\")\n",
    "    \n",
    "    return compact_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Combine essential data\n",
    "    df = combine_essential_data(\n",
    "        input_folder=\"output_by_year\", \n",
    "        output_file=\"essential_newspaper_data.csv\"\n",
    "    )\n",
    "    \n",
    "    if df is not None:\n",
    "        # Optional: Create compact version if file is still too large\n",
    "        file_size_mb = os.path.getsize(\"essential_newspaper_data.csv\") / (1024**2)\n",
    "        if file_size_mb > 100:  # If larger than 100MB\n",
    "            print(f\"\\nFile is {file_size_mb:.2f} MB - creating compact version...\")\n",
    "            compact_df = create_even_more_compact_version(df, \"compact_newspaper_data.csv\")\n",
    "        else:\n",
    "            print(f\"\\nFile size ({file_size_mb:.2f} MB) is manageable.\")\n",
    "        \n",
    "        print(f\"\\nDone! Your essential data is ready for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf679c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSV files with essential data only...\n",
      "Files in 'output_by_year': ['newspaper_data_2002.csv', 'newspaper_data_2003.csv', 'newspaper_data_2004.csv', 'newspaper_data_2005.csv', 'newspaper_data_2006.csv', 'newspaper_data_2007.csv', 'newspaper_data_2008.csv', 'newspaper_data_2009.csv', 'newspaper_data_2010.csv', 'newspaper_data_2011.csv', 'newspaper_data_2012.csv', 'newspaper_data_2013.csv', 'newspaper_data_2014.csv', 'newspaper_data_2015.csv', 'newspaper_data_2016.csv', 'newspaper_data_2017.csv', 'newspaper_data_2018.csv', 'newspaper_data_2019.csv', 'newspaper_data_2020.csv', 'newspaper_data_2021.csv', 'newspaper_data_2022.csv', 'newspaper_data_2023.csv', 'newspaper_data_2024.csv', 'newspaper_data_2025.csv', 'yearly_summary.csv']\n",
      "Found 24 files matching pattern 'newspaper_data_*.csv'\n",
      "Found 24 CSV files to combine...\n",
      "Processing newspaper_data_2002.csv...\n",
      "  Added 10159 articles from 2002\n",
      "Processing newspaper_data_2003.csv...\n",
      "  Added 11340 articles from 2003\n",
      "Processing newspaper_data_2004.csv...\n",
      "  Added 10850 articles from 2004\n",
      "Processing newspaper_data_2005.csv...\n",
      "  Added 10572 articles from 2005\n",
      "Processing newspaper_data_2006.csv...\n",
      "  Added 9805 articles from 2006\n",
      "Processing newspaper_data_2007.csv...\n",
      "  Added 8679 articles from 2007\n",
      "Processing newspaper_data_2008.csv...\n",
      "  Added 8490 articles from 2008\n",
      "Processing newspaper_data_2009.csv...\n",
      "  Added 8023 articles from 2009\n",
      "Processing newspaper_data_2010.csv...\n",
      "  Added 7978 articles from 2010\n",
      "Processing newspaper_data_2011.csv...\n",
      "  Added 7454 articles from 2011\n",
      "Processing newspaper_data_2012.csv...\n",
      "  Added 7717 articles from 2012\n",
      "Processing newspaper_data_2013.csv...\n",
      "  Added 6544 articles from 2013\n",
      "Processing newspaper_data_2014.csv...\n",
      "  Added 6032 articles from 2014\n",
      "Processing newspaper_data_2015.csv...\n",
      "  Added 6148 articles from 2015\n",
      "Processing newspaper_data_2016.csv...\n",
      "  Added 6030 articles from 2016\n",
      "Processing newspaper_data_2017.csv...\n",
      "  Added 5637 articles from 2017\n",
      "Processing newspaper_data_2018.csv...\n",
      "  Added 5683 articles from 2018\n",
      "Processing newspaper_data_2019.csv...\n",
      "  Added 6252 articles from 2019\n",
      "Processing newspaper_data_2020.csv...\n",
      "  Added 5042 articles from 2020\n",
      "Processing newspaper_data_2021.csv...\n",
      "  Added 5345 articles from 2021\n",
      "Processing newspaper_data_2022.csv...\n",
      "  Added 4640 articles from 2022\n",
      "Processing newspaper_data_2023.csv...\n",
      "  Added 5028 articles from 2023\n",
      "Processing newspaper_data_2024.csv...\n",
      "  Added 6392 articles from 2024\n",
      "Processing newspaper_data_2025.csv...\n",
      "  Added 4159 articles from 2025\n",
      "\n",
      "Combining all data...\n",
      "Cleaning data...\n",
      "\n",
      "==================================================\n",
      "ESSENTIAL DATA SUMMARY\n",
      "==================================================\n",
      "Total articles: 173999\n",
      "Years covered: 64 unique years\n",
      "Year range: 20 02 to 2025\n",
      "Unique publications: 257\n",
      "\n",
      "Articles per year:\n",
      "  20 02: 4 articles\n",
      "  20 03: 1 articles\n",
      "  20 04: 2 articles\n",
      "  20 06: 4 articles\n",
      "  20 07: 4 articles\n",
      "  20 09: 1 articles\n",
      "  20 10: 1 articles\n",
      "  20 11: 1 articles\n",
      "  20 12: 1 articles\n",
      "  20 13: 1 articles\n",
      "  20 16: 2 articles\n",
      "  20 17: 3 articles\n",
      "  20 18: 2 articles\n",
      "  20 19: 5 articles\n",
      "  20 21: 8 articles\n",
      "  20 23: 4 articles\n",
      "  20 24: 1 articles\n",
      "  20 25: 2 articles\n",
      "  200 2: 3 articles\n",
      "  200 3: 6 articles\n",
      "  200 4: 2 articles\n",
      "  200 5: 3 articles\n",
      "  200 6: 2 articles\n",
      "  200 7: 2 articles\n",
      "  2002: 10152 articles\n",
      "  2003: 11333 articles\n",
      "  2004: 10846 articles\n",
      "  2005: 10569 articles\n",
      "  2006: 9799 articles\n",
      "  2007: 8673 articles\n",
      "  2008: 12 articles\n",
      "  2008.0: 8478 articles\n",
      "  2009: 8022 articles\n",
      "  201 0: 4 articles\n",
      "  201 1: 2 articles\n",
      "  201 2: 1 articles\n",
      "  201 4: 4 articles\n",
      "  201 5: 1 articles\n",
      "  201 6: 1 articles\n",
      "  201 7: 3 articles\n",
      "  201 8: 2 articles\n",
      "  201 9: 3 articles\n",
      "  2010: 7973 articles\n",
      "  2011: 7451 articles\n",
      "  2012: 7715 articles\n",
      "  2013: 6543 articles\n",
      "  2014: 6028 articles\n",
      "  2015: 6147 articles\n",
      "  2016: 6027 articles\n",
      "  2017: 5631 articles\n",
      "  2018: 5679 articles\n",
      "  2019: 6244 articles\n",
      "  202 0: 4 articles\n",
      "  202 1: 3 articles\n",
      "  202 2: 4 articles\n",
      "  202 3: 4 articles\n",
      "  202 4: 4 articles\n",
      "  202 5: 4 articles\n",
      "  2020: 5038 articles\n",
      "  2021: 5334 articles\n",
      "  2022: 4636 articles\n",
      "  2023: 5020 articles\n",
      "  2024: 6387 articles\n",
      "  2025: 4153 articles\n",
      "\n",
      "Top 10 publications:\n",
      "  Chicago Tribune; Chicago, Ill.: 19852 articles\n",
      "  The Washington Post; Washington, D.C.: 19487 articles\n",
      "  Los Angeles Times; Los Angeles, Calif.: 17182 articles\n",
      "  Boston Globe; Boston, Mass.: 16614 articles\n",
      "  USA TODAY; McLean, Va.: 13455 articles\n",
      "  Philadelphia Inquirer; Philadelphia, Pa.: 13411 articles\n",
      "  Star Tribune; Minneapolis, Minn.: 12638 articles\n",
      "  Arizona Republic; Phoenix, Ariz.: 11132 articles\n",
      "  Newsday, Combined editions; Long Island, N.Y.: 10742 articles\n",
      "  USA Today (Online); Arlington: 10443 articles\n",
      "\n",
      "Data quality:\n",
      "  Articles with titles: 173999\n",
      "  Articles with main text: 173999\n",
      "  Articles with publication name: 173999\n",
      "\n",
      "Output file: essential_newspaper_data.csv\n",
      "File size: 750.77 MB\n",
      "\n",
      "Sample data (first 3 rows):\n",
      " year                                                                                                                               title                              publication_name\n",
      "20 02 A Sudden Tragedy, a Long Recovery; After Illness and Amputations, Virginia Tech's DuBose Discovers a World of Help: [FINAL Edition]         The Washington Post; Washington, D.C.\n",
      "20 02                         Down-home coach leads La. Tech ; Barmore's program short on funds but not short on success: [FINAL Edition]                        USA TODAY; McLean, Va.\n",
      "20 02                                                                  It's a Must for Latrell to Play Well - Next Season: [ALL EDITIONS] Newsday, Combined editions; Long Island, N.Y.\n"
     ]
    }
   ],
   "source": [
    "# Run the combiner - it should work now\n",
    "df = combine_essential_data(\n",
    "    input_folder=\"output_by_year\", \n",
    "    output_file=\"essential_newspaper_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2b2b",
   "metadata": {},
   "source": [
    "##### **© 2024–2025 MD Rafsun Sheikh**\n",
    "##### **Licensed under the Apache License, Version 2.0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e391ecd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

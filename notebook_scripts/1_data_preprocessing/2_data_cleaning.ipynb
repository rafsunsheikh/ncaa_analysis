{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b4e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_duplicates(df, text_similarity_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate analysis for newspaper data\n",
    "    \"\"\"\n",
    "    print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "    \n",
    "    # 1. EXACT DUPLICATES (all fields identical)\n",
    "    print(\"\\n1. EXACT DUPLICATES:\")\n",
    "    exact_dupes = df.duplicated(subset=['title', 'main_text', 'publication_name', 'year'])\n",
    "    exact_dupe_count = exact_dupes.sum()\n",
    "    print(f\"   Exact duplicates: {exact_dupe_count}\")\n",
    "    \n",
    "    if exact_dupe_count > 0:\n",
    "        print(\"   Sample exact duplicates:\")\n",
    "        sample_exact = df[exact_dupes][['year', 'title', 'publication_name']].head(3)\n",
    "        print(sample_exact.to_string(index=False))\n",
    "    \n",
    "    # 2. TITLE + YEAR DUPLICATES (same story, possibly different editions)\n",
    "    print(\"\\n2. SAME TITLE + YEAR (different editions/publications):\")\n",
    "    title_year_dupes = df.duplicated(subset=['title', 'year'], keep=False)\n",
    "    title_year_dupe_count = title_year_dupes.sum()\n",
    "    print(f\"   Articles with same title+year: {title_year_dupe_count}\")\n",
    "    \n",
    "    if title_year_dupe_count > 0:\n",
    "        print(\"   Sample same title+year:\")\n",
    "        sample_title = df[title_year_dupes][['year', 'title', 'publication_name']].head(6)\n",
    "        print(sample_title.to_string(index=False))\n",
    "    \n",
    "    # 3. EDITION VARIANTS (same title with edition markers)\n",
    "    print(\"\\n3. EDITION VARIANTS:\")\n",
    "    edition_patterns = [r'\\[.*Edition.*\\]', r'\\[Final\\]', r'\\[METRO\\]', r'\\[Early\\]']\n",
    "    \n",
    "    # Create title without edition markers\n",
    "    df['title_clean'] = df['title'].astype(str)\n",
    "    for pattern in edition_patterns:\n",
    "        df['title_clean'] = df['title_clean'].str.replace(pattern, '', regex=True)\n",
    "    df['title_clean'] = df['title_clean'].str.strip()\n",
    "    \n",
    "    # Find articles with same clean title + year\n",
    "    clean_title_dupes = df.duplicated(subset=['title_clean', 'year'], keep=False)\n",
    "    clean_title_dupe_count = clean_title_dupes.sum()\n",
    "    edition_variants = clean_title_dupe_count - title_year_dupe_count\n",
    "    print(f\"   Edition variants: {edition_variants}\")\n",
    "    \n",
    "    if edition_variants > 0:\n",
    "        print(\"   Sample edition variants:\")\n",
    "        # Show cases where clean title matches but original title doesn't\n",
    "        mask = clean_title_dupes & ~title_year_dupes\n",
    "        sample_editions = df[mask][['year', 'title', 'publication_name']].head(4)\n",
    "        print(sample_editions.to_string(index=False))\n",
    "    \n",
    "    # # 4. SIMILAR TITLES (typos, minor differences)\n",
    "    # print(\"\\n4. SIMILAR TITLES (potential typos/variations):\")\n",
    "    # similar_titles = find_similar_titles(df, similarity_threshold=0.9)\n",
    "    # print(f\"   Similar title pairs: {len(similar_titles)}\")\n",
    "    \n",
    "    # if similar_titles:\n",
    "    #     print(\"   Sample similar titles:\")\n",
    "    #     for i, (title1, title2, similarity) in enumerate(similar_titles[:3]):\n",
    "    #         print(f\"   {i+1}. Similarity: {similarity:.3f}\")\n",
    "    #         print(f\"      '{title1}'\")\n",
    "    #         print(f\"      '{title2}'\")\n",
    "    \n",
    "    # 5. WIRE SERVICE ARTICLES (same text, different publications)\n",
    "    # print(\"\\n5. POTENTIAL WIRE SERVICE ARTICLES:\")\n",
    "    # wire_articles = find_wire_service_articles(df, text_similarity_threshold)\n",
    "    # print(f\"   Potential wire service duplicates: {len(wire_articles)}\")\n",
    "    \n",
    "    # if wire_articles:\n",
    "    #     print(\"   Sample wire service articles:\")\n",
    "    #     for i, article_group in enumerate(wire_articles[:2]):\n",
    "    #         print(f\"   Group {i+1}: {len(article_group)} publications\")\n",
    "    #         sample_group = df.iloc[article_group[:3]]\n",
    "    #         print(sample_group[['title', 'publication_name']].to_string(index=False))\n",
    "    #         print()\n",
    "    \n",
    "    # 6. PUBLICATION PATTERNS\n",
    "    print(\"\\n6. PUBLICATION PATTERNS:\")\n",
    "    pub_counts = df['publication_name'].value_counts()\n",
    "    print(f\"   Total unique publications: {len(pub_counts)}\")\n",
    "    print(f\"   Top 5 publications by article count:\")\n",
    "    for pub, count in pub_counts.head().items():\n",
    "        print(f\"     {pub}: {count} articles\")\n",
    "    \n",
    "    # 7. YEARLY DISTRIBUTION\n",
    "    print(\"\\n7. YEARLY DISTRIBUTION:\")\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    print(f\"   Articles by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"     {year}: {count} articles\")\n",
    "    \n",
    "    return {\n",
    "        'exact_duplicates': exact_dupe_count,\n",
    "        'title_year_duplicates': title_year_dupe_count,\n",
    "        'edition_variants': edition_variants,\n",
    "        # 'similar_titles': similar_titles,\n",
    "        # 'wire_articles': wire_articles\n",
    "    }\n",
    "\n",
    "def find_similar_titles(df, similarity_threshold=0.9):\n",
    "    \"\"\"Find titles that are very similar (potential typos)\"\"\"\n",
    "    titles = df['title'].dropna().unique()\n",
    "    similar_pairs = []\n",
    "    \n",
    "    for i, title1 in enumerate(titles):\n",
    "        for title2 in titles[i+1:]:\n",
    "            similarity = SequenceMatcher(None, title1.lower(), title2.lower()).ratio()\n",
    "            if similarity >= similarity_threshold and similarity < 1.0:\n",
    "                similar_pairs.append((title1, title2, similarity))\n",
    "    \n",
    "    return sorted(similar_pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def find_wire_service_articles(df, text_similarity_threshold=0.85):\n",
    "    \"\"\"Find articles with similar text across different publications\"\"\"\n",
    "    # Group by title to find potential wire articles\n",
    "    title_groups = df.groupby('title')\n",
    "    wire_articles = []\n",
    "    \n",
    "    for title, group in title_groups:\n",
    "        if len(group) > 1:  # Multiple articles with same title\n",
    "            unique_pubs = group['publication_name'].nunique()\n",
    "            if unique_pubs > 1:  # Across different publications\n",
    "                # Check if text is similar\n",
    "                texts = group['main_text'].dropna()\n",
    "                if len(texts) > 1:\n",
    "                    # Compare first two texts as sample\n",
    "                    text1, text2 = list(texts)[:2]\n",
    "                    if len(text1) > 100 and len(text2) > 100:  # Meaningful text length\n",
    "                        similarity = SequenceMatcher(None, text1[:1000], text2[:1000]).ratio()\n",
    "                        if similarity >= text_similarity_threshold:\n",
    "                            wire_articles.append(group.index.tolist())\n",
    "    \n",
    "    return wire_articles\n",
    "\n",
    "def create_deduplication_strategy(df, analysis_results):\n",
    "    \"\"\"Suggest deduplication strategies based on analysis\"\"\"\n",
    "    print(\"\\n=== DEDUPLICATION RECOMMENDATIONS ===\")\n",
    "    \n",
    "    exact_dupes = analysis_results['exact_duplicates']\n",
    "    title_year_dupes = analysis_results['title_year_duplicates']\n",
    "    edition_variants = analysis_results['edition_variants']\n",
    "    \n",
    "    total_potential_dupes = exact_dupes + title_year_dupes + edition_variants\n",
    "    \n",
    "    print(f\"Total potential duplicates: {total_potential_dupes}\")\n",
    "    print(f\"Potential final dataset size: {len(df) - total_potential_dupes}\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDED DEDUPLICATION STEPS:\")\n",
    "    print(\"1. Remove exact duplicates (safe to remove)\")\n",
    "    print(\"2. For same title+year: keep one per publication\")\n",
    "    print(\"3. For edition variants: keep the most complete version\")\n",
    "    print(\"4. For wire articles: decide if you want multiple publications or just one\")\n",
    "    \n",
    "    return total_potential_dupes\n",
    "\n",
    "def remove_duplicates(df, strategy='conservative'):\n",
    "    \"\"\"\n",
    "    Remove duplicates based on strategy\n",
    "    Conservative: Only remove exact duplicates\n",
    "    Moderate: Remove exact + same title/year/publication \n",
    "    Aggressive: Remove exact + same title/year (keep only one per title/year)\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    print(f\"\\nApplying '{strategy}' deduplication strategy...\")\n",
    "    \n",
    "    if strategy == 'conservative':\n",
    "        # Only remove exact duplicates\n",
    "        df_clean = df.drop_duplicates(subset=['title', 'main_text', 'publication_name', 'year'])\n",
    "        \n",
    "    elif strategy == 'moderate':\n",
    "        # Remove exact duplicates + same title/year/publication\n",
    "        df_clean = df.drop_duplicates(subset=['title', 'year', 'publication_name'])\n",
    "        \n",
    "    elif strategy == 'aggressive':\n",
    "        # Keep only one article per title/year combination\n",
    "        df_clean = df.drop_duplicates(subset=['title', 'year'])\n",
    "        \n",
    "    else:\n",
    "        print(\"Unknown strategy. Using conservative.\")\n",
    "        df_clean = df.drop_duplicates(subset=['title', 'main_text', 'publication_name', 'year'])\n",
    "    \n",
    "    removed_count = original_count - len(df_clean)\n",
    "    print(f\"Removed {removed_count} duplicates ({100*removed_count/original_count:.1f}%)\")\n",
    "    print(f\"Final dataset: {len(df_clean)} articles\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Main execution function\n",
    "def full_duplicate_analysis(csv_file=\"essential_newspaper_data.csv\"):\n",
    "    \"\"\"Complete duplicate analysis workflow\"\"\"\n",
    "    print(\"Loading data for duplicate analysis...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"Loaded {len(df)} articles\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Run analysis\n",
    "    analysis_results = analyze_duplicates(df)\n",
    "    \n",
    "    # Create recommendations\n",
    "    create_deduplication_strategy(df, analysis_results)\n",
    "    \n",
    "    # Show sample deduplication results\n",
    "    print(\"\\n=== SAMPLE DEDUPLICATION RESULTS ===\")\n",
    "    strategies = ['conservative', 'moderate', 'aggressive']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        df_duplicate_removed = remove_duplicates(df.copy(), strategy=strategy)\n",
    "    \n",
    "    return df_duplicate_removed, analysis_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898a4953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for duplicate analysis...\n",
      "Loaded 173999 articles\n",
      "=== DUPLICATE ANALYSIS ===\n",
      "Total articles: 173999\n",
      "\n",
      "1. EXACT DUPLICATES:\n",
      "   Exact duplicates: 11283\n",
      "   Sample exact duplicates:\n",
      " year                                                                    title                       publication_name\n",
      "20 17                                          IN BRIEF [Corrected 06/04/2017]         Chicago Tribune; Chicago, Ill.\n",
      "200 5 GOPHERS UPDATE ; Gophers aiming for sweep at Ohio State: [METRO Edition]       Star Tribune; Minneapolis, Minn.\n",
      " 2002                'A Season on the Brink' Creators Cry Foul: [HOME EDITION] Los Angeles Times; Los Angeles, Calif.\n",
      "\n",
      "2. SAME TITLE + YEAR (different editions/publications):\n",
      "   Articles with same title+year: 37035\n",
      "   Sample same title+year:\n",
      " year                                                                    title                       publication_name\n",
      "20 17                                          IN BRIEF [Corrected 06/04/2017]         Chicago Tribune; Chicago, Ill.\n",
      "20 17                                          IN BRIEF [Corrected 06/04/2017]         Chicago Tribune; Chicago, Ill.\n",
      "200 5 GOPHERS UPDATE ; Gophers aiming for sweep at Ohio State: [METRO Edition]       Star Tribune; Minneapolis, Minn.\n",
      "200 5 GOPHERS UPDATE ; Gophers aiming for sweep at Ohio State: [METRO Edition]       Star Tribune; Minneapolis, Minn.\n",
      " 2002                'A Season on the Brink' Creators Cry Foul: [HOME EDITION] Los Angeles Times; Los Angeles, Calif.\n",
      " 2002                'A Season on the Brink' Creators Cry Foul: [HOME EDITION] Los Angeles Times; Los Angeles, Calif.\n",
      "\n",
      "3. EDITION VARIANTS:\n",
      "   Edition variants: 627\n",
      "   Sample edition variants:\n",
      "year                                                               title                      publication_name\n",
      "2002   'Gut feeling' brought Webb back to train at home: [FINAL Edition]                USA TODAY; McLean, Va.\n",
      "2002 'Gut feeling' brought Webb back to train at home: [FIRST Edition 1]                USA TODAY; McLean, Va.\n",
      "2002          After a Long Climb, Braswell Is Peaking: [FINAL Edition 1] The Washington Post; Washington, D.C.\n",
      "2002            After a Long Climb, Braswell Is Peaking: [FINAL Edition] The Washington Post; Washington, D.C.\n",
      "\n",
      "6. PUBLICATION PATTERNS:\n",
      "   Total unique publications: 256\n",
      "   Top 5 publications by article count:\n",
      "     Chicago Tribune; Chicago, Ill.: 19852 articles\n",
      "     The Washington Post; Washington, D.C.: 19487 articles\n",
      "     Los Angeles Times; Los Angeles, Calif.: 17182 articles\n",
      "     Boston Globe; Boston, Mass.: 16614 articles\n",
      "     USA TODAY; McLean, Va.: 13455 articles\n",
      "\n",
      "7. YEARLY DISTRIBUTION:\n",
      "   Articles by year:\n",
      "     20 02: 4 articles\n",
      "     20 03: 1 articles\n",
      "     20 04: 2 articles\n",
      "     20 06: 4 articles\n",
      "     20 07: 4 articles\n",
      "     20 09: 1 articles\n",
      "     20 10: 1 articles\n",
      "     20 11: 1 articles\n",
      "     20 12: 1 articles\n",
      "     20 13: 1 articles\n",
      "     20 16: 2 articles\n",
      "     20 17: 3 articles\n",
      "     20 18: 2 articles\n",
      "     20 19: 5 articles\n",
      "     20 21: 8 articles\n",
      "     20 23: 4 articles\n",
      "     20 24: 1 articles\n",
      "     20 25: 2 articles\n",
      "     200 2: 3 articles\n",
      "     200 3: 6 articles\n",
      "     200 4: 2 articles\n",
      "     200 5: 3 articles\n",
      "     200 6: 2 articles\n",
      "     200 7: 2 articles\n",
      "     2002: 10152 articles\n",
      "     2003: 11333 articles\n",
      "     2004: 10846 articles\n",
      "     2005: 10569 articles\n",
      "     2006: 9799 articles\n",
      "     2007: 8673 articles\n",
      "     2008: 12 articles\n",
      "     2008.0: 8478 articles\n",
      "     2009: 8022 articles\n",
      "     201 0: 4 articles\n",
      "     201 1: 2 articles\n",
      "     201 2: 1 articles\n",
      "     201 4: 4 articles\n",
      "     201 5: 1 articles\n",
      "     201 6: 1 articles\n",
      "     201 7: 3 articles\n",
      "     201 8: 2 articles\n",
      "     201 9: 3 articles\n",
      "     2010: 7973 articles\n",
      "     2011: 7451 articles\n",
      "     2012: 7715 articles\n",
      "     2013: 6543 articles\n",
      "     2014: 6028 articles\n",
      "     2015: 6147 articles\n",
      "     2016: 6027 articles\n",
      "     2017: 5631 articles\n",
      "     2018: 5679 articles\n",
      "     2019: 6244 articles\n",
      "     202 0: 4 articles\n",
      "     202 1: 3 articles\n",
      "     202 2: 4 articles\n",
      "     202 3: 4 articles\n",
      "     202 4: 4 articles\n",
      "     202 5: 4 articles\n",
      "     2020: 5038 articles\n",
      "     2021: 5334 articles\n",
      "     2022: 4636 articles\n",
      "     2023: 5020 articles\n",
      "     2024: 6387 articles\n",
      "     2025: 4153 articles\n",
      "\n",
      "=== DEDUPLICATION RECOMMENDATIONS ===\n",
      "Total potential duplicates: 48945\n",
      "Potential final dataset size: 125054\n",
      "\n",
      "RECOMMENDED DEDUPLICATION STEPS:\n",
      "1. Remove exact duplicates (safe to remove)\n",
      "2. For same title+year: keep one per publication\n",
      "3. For edition variants: keep the most complete version\n",
      "4. For wire articles: decide if you want multiple publications or just one\n",
      "\n",
      "=== SAMPLE DEDUPLICATION RESULTS ===\n",
      "\n",
      "Applying 'conservative' deduplication strategy...\n",
      "Removed 11283 duplicates (6.5%)\n",
      "Final dataset: 162716 articles\n",
      "\n",
      "Applying 'moderate' deduplication strategy...\n",
      "Removed 23927 duplicates (13.8%)\n",
      "Final dataset: 150072 articles\n",
      "\n",
      "Applying 'aggressive' deduplication strategy...\n",
      "Removed 24057 duplicates (13.8%)\n",
      "Final dataset: 149942 articles\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_duplicate_removed, results = full_duplicate_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbc6dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149942"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_duplicate_removed)  # Check the length of the final deduplicated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c42e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved to: cleaned_newspaper_data.csv\n",
      "📊 Dataset summary:\n",
      "   - Original articles: 173,999\n",
      "   - Duplicates removed: 24,057\n",
      "   - Final articles: 149,942\n",
      "   - Reduction: 13.8%\n",
      "   - File size: 673.47 MB\n",
      "\n",
      "📋 Quick verification:\n",
      "   - Columns: ['year', 'date', 'title', 'main_text', 'publication_name', 'title_clean']\n",
      "   - Year range: 20 02 to 2025\n",
      "   - Unique publications: 248\n",
      "\n",
      "📝 Sample of cleaned data:\n",
      " year                                                                                                                               title                              publication_name\n",
      "20 02 A Sudden Tragedy, a Long Recovery; After Illness and Amputations, Virginia Tech's DuBose Discovers a World of Help: [FINAL Edition]         The Washington Post; Washington, D.C.\n",
      "20 02                         Down-home coach leads La. Tech ; Barmore's program short on funds but not short on success: [FINAL Edition]                        USA TODAY; McLean, Va.\n",
      "20 02                                                                  It's a Must for Latrell to Play Well - Next Season: [ALL EDITIONS] Newsday, Combined editions; Long Island, N.Y.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataframe to a new CSV file\n",
    "output_filename = \"cleaned_newspaper_data.csv\"\n",
    "df_duplicate_removed.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✅ Cleaned dataset saved to: {output_filename}\")\n",
    "print(f\"📊 Dataset summary:\")\n",
    "print(f\"   - Original articles: 173,999\")\n",
    "print(f\"   - Duplicates removed: 24,057\")\n",
    "print(f\"   - Final articles: 149,942\")\n",
    "print(f\"   - Reduction: {100 * 24057 / 173999:.1f}%\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size_mb = os.path.getsize(output_filename) / (1024**2)\n",
    "print(f\"   - File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Quick verification of the cleaned data\n",
    "print(f\"\\n📋 Quick verification:\")\n",
    "print(f\"   - Columns: {list(df_duplicate_removed.columns)}\")\n",
    "print(f\"   - Year range: {df_duplicate_removed['year'].min()} to {df_duplicate_removed['year'].max()}\")\n",
    "print(f\"   - Unique publications: {df_duplicate_removed['publication_name'].nunique()}\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(f\"\\n📝 Sample of cleaned data:\")\n",
    "sample = df_duplicate_removed[['year', 'title', 'publication_name']].head(3)\n",
    "print(sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0d9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Detailed report saved as: cleaning_report.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a cleaning summary report\n",
    "from datetime import datetime\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "NEWSPAPER DATA CLEANING REPORT\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "ORIGINAL DATASET:\n",
    "- Total articles: 173,999\n",
    "- Years covered: {df_duplicate_removed['year'].min()} - {df_duplicate_removed['year'].max()}\n",
    "\n",
    "DUPLICATES REMOVED:\n",
    "- Exact duplicates: [number from your analysis]\n",
    "- Same title + year: [number from your analysis] \n",
    "- Edition variants: [number from your analysis]\n",
    "- Total removed: 24,057\n",
    "\n",
    "FINAL CLEAN DATASET:\n",
    "- Total articles: 149,942\n",
    "- Reduction: 13.8%\n",
    "- Unique publications: {df_duplicate_removed['publication_name'].nunique()}\n",
    "- File: cleaned_newspaper_data.csv\n",
    "- Size: {file_size_mb:.2f} MB\n",
    "\n",
    "YEARLY DISTRIBUTION:\n",
    "{df_duplicate_removed['year'].value_counts().sort_index().to_string()}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"cleaning_report.txt\", \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"📄 Detailed report saved as: cleaning_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a869a",
   "metadata": {},
   "source": [
    "##### **© 2024–2025 MD Rafsun Sheikh**\n",
    "##### **Licensed under the Apache License, Version 2.0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2e283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
